import os
import base64
import streamlit as st
import pandas as pd
import numpy as np
import requests
import json

# â”€â”€â”€ Lazy import for psycopg2 (may not be available in all environments) â”€â”€â”€â”€â”€
_psycopg2 = None
_psycopg2_error = None

def _get_psycopg2():
    """Lazy-load psycopg2 on first use; cache the result."""
    global _psycopg2, _psycopg2_error
    if _psycopg2 is not None:
        return _psycopg2
    if _psycopg2_error is not None:
        return None
    try:
        import psycopg2 as _pg
        _psycopg2 = _pg
        return _psycopg2
    except ImportError as exc:
        _psycopg2_error = str(exc)
        return None

# â”€â”€â”€ Page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Actuarial Risk Dashboard",
    page_icon="ğŸ“Š",
    layout="wide",
)

# â”€â”€â”€ Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_workspace_client = None
_auth_init_error = None

def _get_workspace_client():
    """Lazily initialise the Databricks WorkspaceClient."""
    global _workspace_client, _auth_init_error
    if _workspace_client is not None:
        return _workspace_client
    if _auth_init_error is not None:
        return None
    try:
        from databricks import sdk
        _workspace_client = sdk.WorkspaceClient()
        return _workspace_client
    except Exception as exc:
        _auth_init_error = str(exc)
        return None

def get_token():
    """Get OAuth token for user-context operations (Lakebase, model serving).
    Prefers the forwarded user token, falls back to the SDK service principal token."""
    user_token = st.context.headers.get("X-Forwarded-Access-Token")
    if user_token:
        return user_token
    w = _get_workspace_client()
    if w is not None:
        try:
            if w.config.token:
                return w.config.token
            headers = {}
            w.config.authenticate(headers)
            if "Authorization" in headers:
                return headers["Authorization"].replace("Bearer ", "")
        except Exception:
            pass
    return None

# â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WORKSPACE_HOST = os.environ.get("DATABRICKS_HOST", "")
if WORKSPACE_HOST and not WORKSPACE_HOST.startswith("http"):
    WORKSPACE_HOST = f"https://{WORKSPACE_HOST}"

WAREHOUSE_ID  = os.environ.get("DATABRICKS_WAREHOUSE_ID", "")

# CATALOG / SCHEMA / PG_DATABASE / ENDPOINT_NAME / LAKEBASE_ENDPOINT_PATH come
# from app/_bundle_config.py, generated by deploy.sh before each deploy.
try:
    from _bundle_config import (
        CATALOG as _BC_CATALOG,
        SCHEMA as _BC_SCHEMA,
        PG_DATABASE as _BC_PG,
        ENDPOINT_NAME as _BC_ENDPOINT,
        LAKEBASE_ENDPOINT_PATH as _BC_LAKEBASE_PATH,
        LAKEBASE_HOST as _BC_LAKEBASE_HOST,
    )
    try:
        from _bundle_config import MC_ENDPOINT_NAME as _BC_MC_ENDPOINT
    except ImportError:
        _BC_MC_ENDPOINT = "actuarial-workshop-monte-carlo"
except ImportError:
    _BC_CATALOG = "my_catalog"
    _BC_SCHEMA = "actuarial_workshop"
    _BC_PG = "actuarial_workshop_db"
    _BC_ENDPOINT = "actuarial-workshop-sarima-forecaster"
    _BC_MC_ENDPOINT = "actuarial-workshop-monte-carlo"
    _BC_LAKEBASE_PATH = "projects/actuarial-workshop-lakebase/branches/main/endpoints/primary"
    _BC_LAKEBASE_HOST = ""

CATALOG                = os.environ.get("CATALOG") or _BC_CATALOG
SCHEMA                 = os.environ.get("SCHEMA") or _BC_SCHEMA
PG_DATABASE_DEFAULT    = _BC_PG
ENDPOINT_NAME          = os.environ.get("ENDPOINT_NAME") or _BC_ENDPOINT
MC_ENDPOINT_NAME       = os.environ.get("MC_ENDPOINT_NAME") or _BC_MC_ENDPOINT
LAKEBASE_ENDPOINT_PATH = os.environ.get("LAKEBASE_ENDPOINT_PATH") or _BC_LAKEBASE_PATH

if _auth_init_error is not None:
    st.warning(
        f"Databricks SDK could not initialise: {_auth_init_error}. "
        "Data features will be unavailable until the issue is resolved."
    )

# â”€â”€â”€ SQL Execution Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def execute_sql(statement: str) -> pd.DataFrame:
    """Execute SQL via the Databricks SDK StatementExecution â€” handles auth automatically."""
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return pd.DataFrame()
    try:
        result = w.statement_execution.execute_statement(
            warehouse_id=WAREHOUSE_ID,
            statement=statement,
            wait_timeout="30s",
        )
        state = result.status.state.value if result.status and result.status.state else "UNKNOWN"
        if state == "SUCCEEDED":
            columns = []
            if result.manifest and result.manifest.schema and result.manifest.schema.columns:
                columns = [c.name for c in result.manifest.schema.columns]
            rows = []
            if result.result and result.result.data_array:
                rows = result.result.data_array
            if columns and rows:
                return pd.DataFrame(rows, columns=columns)
            return pd.DataFrame()
        else:
            error_msg = "Unknown SQL error"
            if result.status and result.status.error:
                error_msg = result.status.error.message or error_msg
            st.error(f"SQL Error ({state}): {error_msg}")
            return pd.DataFrame()
    except Exception as exc:
        st.error(f"SQL execution failed: {exc}")
        return pd.DataFrame()

# â”€â”€â”€ Data Loading Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_data(ttl=300)
def load_segments():
    df = execute_sql(f"SELECT DISTINCT segment_id FROM {CATALOG}.{SCHEMA}.sarima_forecasts ORDER BY 1")
    if not df.empty:
        return df["segment_id"].tolist()
    return []

@st.cache_data(ttl=300)
def load_forecasts(segment_id: str):
    return execute_sql(f"""
        SELECT month, record_type, claims_count, forecast_mean, forecast_lo95, forecast_hi95
        FROM {CATALOG}.{SCHEMA}.sarima_forecasts
        WHERE segment_id = '{segment_id}'
        ORDER BY month
    """)

@st.cache_data(ttl=600)
def load_monte_carlo_summary():
    df = execute_sql(f"""
        SELECT
            AVG(mean_loss_M)  AS expected_loss,
            AVG(var_99_M)     AS var_99,
            AVG(var_995_M)    AS var_995,
            AVG(cvar_99_M)    AS cvar_99,
            MAX(max_loss_M)   AS max_loss
        FROM {CATALOG}.{SCHEMA}.monte_carlo_results
    """)
    if not df.empty:
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df.iloc[0]
    return pd.Series({
        'expected_loss': 0, 'var_99': 0, 'var_995': 0, 'cvar_99': 0, 'max_loss': 0
    })

@st.cache_data(ttl=600)
def load_monte_carlo_distribution():
    """Load per-simulation total loss for portfolio loss distribution chart."""
    return execute_sql(f"""
        SELECT mean_loss_M, var_99_M, var_995_M, cvar_99_M, max_loss_M
        FROM {CATALOG}.{SCHEMA}.monte_carlo_results
        ORDER BY mean_loss_M
    """)

@st.cache_data(ttl=600)
def load_segment_stats(segment_id: str):
    """Summary statistics for the selected segment's history."""
    return execute_sql(f"""
        SELECT
            MIN(month)                          AS first_month,
            MAX(month)                          AS last_month,
            COUNT(*)                            AS num_months,
            ROUND(AVG(claims_count), 1)         AS avg_monthly_claims,
            ROUND(STDDEV(claims_count), 1)      AS stddev_claims,
            MIN(claims_count)                   AS min_claims,
            MAX(claims_count)                   AS max_claims
        FROM {CATALOG}.{SCHEMA}.gold_claims_monthly
        WHERE segment_id = '{segment_id}'
    """)

@st.cache_data(ttl=600)
def load_stress_scenarios():
    """Load pre-computed stress test scenario comparison from Module 4."""
    df = execute_sql(f"""
        SELECT scenario_label, total_mean_M, var_99_M, var_995_M, cvar_99_M, var_995_vs_baseline
        FROM {CATALOG}.{SCHEMA}.stress_test_scenarios
        ORDER BY var_995_M DESC
    """)
    if not df.empty:
        for col in ['total_mean_M', 'var_99_M', 'var_995_M', 'cvar_99_M', 'var_995_vs_baseline']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

@st.cache_data(ttl=600)
def load_var_timeline():
    """Load the 12-month SARIMA-driven VaR evolution from Module 4."""
    df = execute_sql(f"""
        SELECT forecast_month, month_idx, total_mean_M, var_99_M, var_995_M, cvar_99_M, var_995_vs_baseline
        FROM {CATALOG}.{SCHEMA}.portfolio_risk_timeline
        ORDER BY month_idx
    """)
    if not df.empty:
        for col in ['total_mean_M', 'var_99_M', 'var_995_M', 'cvar_99_M', 'var_995_vs_baseline', 'month_idx']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def call_serving_endpoint(horizon: int) -> pd.DataFrame:
    """Call SARIMA Model Serving endpoint via Databricks SDK."""
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return pd.DataFrame()
    try:
        response = w.serving_endpoints.query(
            name=ENDPOINT_NAME,
            dataframe_records=[{"horizon": horizon}],
        )
        predictions = response.predictions
        if predictions:
            return pd.DataFrame(predictions)
        return pd.DataFrame()
    except Exception as exc:
        st.warning(f"Endpoint unavailable: {exc}")
        return pd.DataFrame()

def call_monte_carlo_endpoint(scenario: dict) -> dict | None:
    """Call the Monte Carlo serving endpoint with a scenario parameter dict.

    Returns a dict of risk metrics or None on error.
    """
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return None
    try:
        response = w.serving_endpoints.query(
            name=MC_ENDPOINT_NAME,
            dataframe_records=[scenario],
        )
        predictions = response.predictions
        if predictions:
            p = predictions[0] if isinstance(predictions, list) else predictions
            return {k: float(v) if k != "copula" else v for k, v in p.items()}
        return None
    except Exception as exc:
        st.warning(f"Monte Carlo endpoint unavailable: {exc}")
        return None

# â”€â”€â”€ Lakebase: Scenario annotations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _email_from_token(token: str) -> str:
    """Decode the JWT payload (without verification) to extract the user identity."""
    try:
        payload_b64 = token.split(".")[1]
        payload_b64 += "=" * (4 - len(payload_b64) % 4)
        payload = json.loads(base64.urlsafe_b64decode(payload_b64))
        return payload.get("sub", payload.get("email", ""))
    except Exception:
        return ""

_lakebase_cred_cache: dict = {"token": None, "expires_at": 0.0}

def _get_lakebase_host() -> str:
    """Return the Lakebase endpoint hostname.

    PGHOST env var takes precedence (can be overridden at runtime).
    Falls back to LAKEBASE_HOST from _bundle_config.py, which is written by
    deploy.sh after lakebase_setup.py resolves and confirms the hostname.
    """
    return os.environ.get("PGHOST") or _BC_LAKEBASE_HOST

def _get_lakebase_credential() -> tuple:
    """Return (client_id, token) for the app SP via generate_database_credential.

    Uses w.postgres.generate_database_credential() (Databricks SDK >= 0.81.0).
    The token is used as the Postgres password â€” Lakebase Autoscaling validates it
    via the databricks_auth extension. Credentials are cached until 60 s before expiry.

    DATABRICKS_CLIENT_ID is injected at runtime by the Databricks Apps platform.
    """
    import time
    now = time.time()
    client_id = os.environ.get("DATABRICKS_CLIENT_ID", "")
    if _lakebase_cred_cache["token"] and _lakebase_cred_cache["expires_at"] > now + 60:
        return client_id, _lakebase_cred_cache["token"]

    w = _get_workspace_client()
    if w is None:
        return client_id, None
    try:
        # SDK >= 0.81.0: w.postgres.generate_database_credential(endpoint=<resource_path>)
        cred = w.postgres.generate_database_credential(endpoint=LAKEBASE_ENDPOINT_PATH)
        token = cred.token
    except AttributeError:
        # Fallback: low-level API call (older SDK versions without w.postgres)
        try:
            cred = w.api_client.do(
                "POST",
                "/api/2.0/postgres/generateDatabaseCredential",
                body={"endpoint": LAKEBASE_ENDPOINT_PATH},
            )
            token = cred.get("token", "") if isinstance(cred, dict) else ""
        except Exception:
            return client_id, None
    except Exception:
        return client_id, None
    # Credentials typically expire in 1 hour; cache conservatively
    _lakebase_cred_cache["token"] = token
    _lakebase_cred_cache["expires_at"] = now + 3300  # 55 min
    return client_id, token


def get_lakebase_conn():
    """Connect to the Lakebase Autoscaling endpoint as the app service principal.

    Authentication uses generate_database_credential â€” the returned token is used
    directly as the Postgres password. The SP role is created by databricks_create_role()
    in app_setup.py (Task 7 of the setup job).

    All DB operations run as the SP. The human analyst's identity is captured from
    the forwarded user token and stored in the `analyst` column.
    """
    psycopg2 = _get_psycopg2()
    if psycopg2 is None:
        raise RuntimeError(
            f"psycopg2 is not available ({_psycopg2_error or 'unknown reason'}). "
            "Lakebase features are disabled."
        )

    host = _get_lakebase_host()
    if not host:
        raise RuntimeError(
            "Lakebase hostname not configured. "
            "Re-run deploy.sh to regenerate app/_bundle_config.py with LAKEBASE_HOST, "
            "or set the PGHOST environment variable."
        )

    sp_user, sp_token = _get_lakebase_credential()
    if not sp_token:
        raise RuntimeError("Could not obtain Lakebase credential â€” check DATABRICKS_CLIENT_ID env var.")

    port     = int(os.environ.get("PGPORT", "5432"))
    database = PG_DATABASE_DEFAULT
    sslmode  = os.environ.get("PGSSLMODE", "require")
    return psycopg2.connect(
        host=host, port=port, database=database,
        user=sp_user, password=sp_token, sslmode=sslmode,
    )

_annotations_table_ensured = False

def _ensure_annotations_table():
    """Create scenario_annotations if it doesn't exist (idempotent, runs once per process)."""
    global _annotations_table_ensured
    if _annotations_table_ensured:
        return
    try:
        conn = get_lakebase_conn()
        cur = conn.cursor()
        # Explicit public schema â€” Lakebase sets search_path = "$user", public so an
        # unqualified name would land in the connecting user's personal schema instead.
        cur.execute("""
            CREATE TABLE IF NOT EXISTS public.scenario_annotations (
                id              SERIAL        PRIMARY KEY,
                segment_id      TEXT          NOT NULL,
                note            TEXT,
                analyst         TEXT,
                scenario_type   TEXT,
                adjustment_pct  NUMERIC(10,2),
                approval_status TEXT          DEFAULT 'Draft',
                created_at      TIMESTAMP     DEFAULT NOW()
            )
        """)
        conn.commit()
        conn.close()
        _annotations_table_ensured = True
    except Exception:
        pass  # Non-fatal â€” table may already exist or Lakebase may be unavailable

_SCENARIO_TYPES = [
    "Assumption Override",
    "Catastrophe Event",
    "External Event",
    "Review Comment",
    "Judgment Adjustment",
    "Model Calibration",
]

_APPROVAL_STATUSES = ["Draft", "Pending Review", "Approved"]

# â”€â”€â”€ CAT scenario presets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Each preset defines severity multipliers (relative to baseline) applied to
# means, CVs, and correlation offsets.  Return period further scales means.
_CAT_PRESETS = {
    "Hurricane":           {
        "means_mult": [2.8, 1.9, 1.3], "cv_mult": [1.4, 1.3, 1.2], "corr_add": 0.15,
        "desc": "Severe wind + storm surge. Property and Auto heavily impacted.",
    },
    "Earthquake":          {
        "means_mult": [3.8, 1.6, 1.4], "cv_mult": [1.6, 1.2, 1.3], "corr_add": 0.20,
        "desc": "Structural damage + fires. Property losses dominate.",
    },
    "Flood":               {
        "means_mult": [2.3, 1.7, 1.1], "cv_mult": [1.3, 1.2, 1.1], "corr_add": 0.12,
        "desc": "Widespread inundation. Both Property and Auto impacted.",
    },
    "Wildfire":            {
        "means_mult": [3.2, 1.4, 1.2], "cv_mult": [1.5, 1.1, 1.2], "corr_add": 0.15,
        "desc": "Structure and vehicle losses across a broad geographic area.",
    },
    "Pandemic":            {
        "means_mult": [1.1, 0.8, 2.8], "cv_mult": [1.2, 1.1, 1.5], "corr_add": 0.25,
        "desc": "Liability and healthcare claims surge; Auto frequency drops.",
    },
    "Market Crash":        {
        "means_mult": [1.2, 1.1, 1.8], "cv_mult": [1.3, 1.2, 1.4], "corr_add": 0.30,
        "desc": "Systemic financial stress â€” correlated across all lines.",
    },
    "Industrial Accident": {
        "means_mult": [2.0, 1.5, 2.2], "cv_mult": [1.3, 1.2, 1.4], "corr_add": 0.18,
        "desc": "Explosion, chemical spill, or structural collapse. High liability.",
    },
}

_RETURN_PERIOD_MULT = {
    "1-in-50yr":  0.70,
    "1-in-100yr": 1.00,
    "1-in-250yr": 1.80,
    "1-in-500yr": 3.00,
}

_CANADIAN_PROVINCES = [
    "Ontario", "Quebec", "British Columbia", "Alberta", "Atlantic",
    "Manitoba", "Saskatchewan", "Nova Scotia", "New Brunswick",
    "Newfoundland", "PEI", "NWT", "Yukon",
]

def save_scenario_annotation(
    segment_id: str,
    note: str,
    analyst: str,
    scenario_type: str = "",
    adjustment_pct: float | None = None,
    approval_status: str = "Draft",
):
    try:
        _ensure_annotations_table()
        conn = get_lakebase_conn()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO public.scenario_annotations "
            "(segment_id, note, analyst, scenario_type, adjustment_pct, approval_status, created_at) "
            "VALUES (%s, %s, %s, %s, %s, %s, NOW())",
            (segment_id, note, analyst,
             scenario_type or None,
             adjustment_pct,
             approval_status or "Draft"),
        )
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        st.warning(f"Could not save annotation: {e}")
        return False

def load_annotations(segment_id: str):
    try:
        _ensure_annotations_table()
        conn = get_lakebase_conn()
        cur = conn.cursor()
        cur.execute(
            "SELECT analyst, scenario_type, adjustment_pct, approval_status, note, created_at "
            "FROM public.scenario_annotations "
            "WHERE segment_id = %s ORDER BY created_at DESC LIMIT 20",
            (segment_id,)
        )
        rows = cur.fetchall()
        conn.close()
        if rows:
            return pd.DataFrame(
                rows,
                columns=["Analyst", "Type", "Adj %", "Status", "Note", "Created At"],
            )
        return pd.DataFrame()
    except Exception:
        return pd.DataFrame()

# â”€â”€â”€ Sidebar: Data & Model Reference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.header("About This Dashboard")

    st.markdown("**Source Data**")
    st.markdown("""
Synthetic insurance claims time series across **52 segments**
(product line Ã— Canadian province/territory):

| Dimension | Values |
|---|---|
| Product lines | Commercial Auto, Commercial Property, Personal Auto, Homeowners |
| Provinces/Territories | Ontario, Quebec, BC, Alberta, Atlantic, Manitoba, Saskatchewan, Nova Scotia, New Brunswick, Newfoundland, PEI, NWT, Yukon |
| History | Jan 2019 â€“ Dec 2024 |
| Granularity | Monthly claims counts |
| Observations | 72 months Ã— 52 segments = 3,744 rows |

Each row records the monthly **claims count** for one segment.
Rolling features (3/6/12-month means and volatility) are
pre-computed in the Feature Store for model training.
""")

    st.divider()
    st.markdown("**Models**")
    st.markdown("""
| Model | What it does |
|---|---|
| **SARIMA** | Seasonal ARIMA fitted independently per segment. Captures trend, seasonality, and autocorrelation in monthly claim counts. Served via REST endpoint (On-Demand Forecast tab). |
| **GARCH(1,1)** | Models time-varying volatility in loss ratios. Useful for risk capital estimation when variance is not constant. |
| **Monte Carlo** | **640M paths** (64 Ray tasks Ã— 10M each) on NVIDIA T4 GPU: baseline + 12-month SARIMA-driven VaR evolution + 3 stress scenarios (CAT event, systemic risk, inflation shock). t-Copula (df=4) + lognormal marginals + Poisson jump shocks for CAT. See **Catastrophe Scenarios** tab. |
""")

    st.divider()
    st.markdown("**Pipeline**")
    st.markdown("""
```
Raw CDC events
  â†’ Bronze (Delta Live Tables)
  â†’ Silver (SCD Type 2 policies)
  â†’ Gold (monthly segment stats)
  â†’ Feature Store (point-in-time joins)
  â†’ SARIMA / GARCH per segment
  â†’ Monte Carlo portfolio simulation
  â†’ UC Model Registry (@Champion)
     â”œâ”€â”€ SARIMA endpoint (Forecasts tab)
     â””â”€â”€ Monte Carlo endpoint (Scenario tab)
  â†’ This App
```
All assets are version-controlled and reproducible.
Model artifacts are logged to MLflow and promoted via
the Unity Catalog Model Registry.
""")

    st.divider()
    st.caption("Powered by Databricks Apps + Streamlit")

# â”€â”€â”€ App Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ“Š Actuarial Risk Dashboard")
st.caption("Powered by Databricks | SARIMA Forecasting + Monte Carlo Portfolio Risk")

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ”® Forecasts", "ğŸ“‰ Portfolio Risk", "âš¡ On-Demand Forecast", "ğŸ² Scenario Analysis", "ğŸŒªï¸ Catastrophe Scenarios"])

# â”€â”€ Tab 1: Segment Forecasts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    st.subheader("SARIMA Claims Forecasts by Segment")

    with st.expander("â„¹ï¸ How this forecast works", expanded=False):
        st.markdown("""
**Model:** SARIMA (Seasonal AutoRegressive Integrated Moving Average), fitted independently
per segment using `statsmodels.SARIMAX`.

**What SARIMA captures:**
- **AR (AutoRegressive):** Each month's claim count is partly explained by prior months â€”
  a high-claims month tends to be followed by another high-claims month.
- **I (Integrated):** Differencing removes trends so the series is stationary before fitting.
- **MA (Moving Average):** Residual shocks (e.g., a one-time weather event) decay over time
  rather than persisting indefinitely.
- **Seasonal (S):** A 12-month seasonal cycle accounts for recurring annual patterns
  (e.g., winter driving claims peak in Q1).

**Order used:** SARIMA(1,1,1)(1,1,1)â‚â‚‚ â€” a conservative, widely applicable specification.

**Confidence interval:** The shaded band is the **95% prediction interval** â€” the range within
which the model expects 95% of actual future observations to fall, accounting for both
parameter uncertainty and inherent randomness in the claims process.

**Limitations:** SARIMA assumes the historical seasonal pattern continues. It will not
anticipate structural breaks (e.g., a new product launch, regulatory change, or pandemic shock).
Use the annotation tool below to flag such assumptions.
""")

    segments = load_segments()
    if segments:
        selected = st.selectbox("Select segment:", segments, index=0)

        if selected:
            # Load segment history stats alongside the forecast
            stats_df = load_segment_stats(selected)
            if not stats_df.empty:
                for col in stats_df.columns:
                    stats_df[col] = stats_df[col].apply(
                        lambda x: pd.to_numeric(x, errors='ignore') if x is not None else x
                    )
                s = stats_df.iloc[0]
                sc1, sc2, sc3, sc4 = st.columns(4)
                sc1.metric(
                    "History",
                    f"{str(s['first_month'])[:7]} â€“ {str(s['last_month'])[:7]}",
                    help="Date range of observed actuals used to fit the SARIMA model"
                )
                sc2.metric(
                    "Avg Monthly Claims",
                    f"{float(s['avg_monthly_claims']):,.0f}",
                    help="Mean monthly claims count over the observed history"
                )
                sc3.metric(
                    "Std Dev",
                    f"{float(s['stddev_claims']):,.0f}",
                    help="Standard deviation of monthly claims â€” higher values indicate greater volatility"
                )
                sc4.metric(
                    "Range",
                    f"{int(float(s['min_claims'])):,} â€“ {int(float(s['max_claims'])):,}",
                    help="Minimum and maximum observed monthly claims count"
                )

            df = load_forecasts(selected)

            if not df.empty:
                for col in ["claims_count", "forecast_mean", "forecast_lo95", "forecast_hi95"]:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                actuals   = df[df["record_type"] == "actual"]
                forecasts = df[df["record_type"] == "forecast"]

                import plotly.graph_objects as go
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=actuals["month"], y=actuals["claims_count"],
                    mode="lines+markers", name="Actual claims",
                    line=dict(color="#1f77b4"),
                    hovertemplate="<b>%{x}</b><br>Actual: %{y:,.0f} claims<extra></extra>",
                ))
                fig.add_trace(go.Scatter(
                    x=forecasts["month"], y=forecasts["forecast_mean"],
                    mode="lines+markers", name="SARIMA forecast (mean)",
                    line=dict(color="#FF3419", dash="dash"),
                    hovertemplate="<b>%{x}</b><br>Forecast: %{y:,.0f} claims<extra></extra>",
                ))
                fig.add_trace(go.Scatter(
                    x=pd.concat([forecasts["month"], forecasts["month"][::-1]]),
                    y=pd.concat([forecasts["forecast_hi95"], forecasts["forecast_lo95"][::-1]]),
                    fill="toself", fillcolor="rgba(255,52,25,0.15)",
                    line=dict(color="rgba(255,0,0,0)"),
                    name="95% prediction interval",
                    hoverinfo="skip",
                ))
                # Add a vertical marker at the forecast start
                if not actuals.empty and not forecasts.empty:
                    cutoff = str(actuals["month"].max())
                    fig.add_shape(
                        type="line",
                        x0=cutoff, x1=cutoff, y0=0, y1=1, yref="paper",
                        line=dict(dash="dot", color="grey", width=1),
                    )
                    fig.add_annotation(
                        x=cutoff, y=1, yref="paper",
                        text="Forecast start", showarrow=False,
                        yanchor="bottom", font=dict(color="grey", size=11),
                    )
                fig.update_layout(
                    title=f"{selected} â€” SARIMA Forecast",
                    xaxis_title="Month",
                    yaxis_title="Monthly Claims Count",
                    height=420,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                    hovermode="x unified",
                )
                st.plotly_chart(fig, use_container_width=True)

                col1, col2, col3 = st.columns(3)
                if "mape" in df.columns:
                    mape_vals = pd.to_numeric(df[df["record_type"] == "forecast"]["mape"], errors='coerce')
                    col1.metric(
                        "Avg MAPE (hold-out)",
                        f"{mape_vals.mean():.1f}%",
                        help="Mean Absolute Percentage Error on the hold-out test set. Lower is better; <10% is considered good for insurance time series."
                    )
                if not forecasts.empty:
                    col2.metric(
                        "Avg Monthly Forecast",
                        f"{int(forecasts['forecast_mean'].mean()):,}",
                        help="Average of the 12 monthly point forecasts (mean of the predictive distribution)"
                    )
                    half_width = int((forecasts['forecast_hi95'] - forecasts['forecast_lo95']).mean() / 2)
                    col3.metric(
                        "Avg 95% CI Half-Width",
                        f"Â±{half_width:,}",
                        help="Average half-width of the 95% prediction interval. Wider intervals reflect greater uncertainty â€” typical for longer-horizon forecasts."
                    )

                with st.expander("ğŸ“‹ Raw forecast data"):
                    st.markdown("""
| Column | Description |
|---|---|
| `month` | Calendar month (YYYY-MM-DD, first of month) |
| `record_type` | `actual` = observed history; `forecast` = SARIMA prediction |
| `claims_count` | Observed monthly claims count (actuals only) |
| `forecast_mean` | Point forecast â€” mean of the predictive distribution |
| `forecast_lo95` | Lower bound of 95% prediction interval |
| `forecast_hi95` | Upper bound of 95% prediction interval |
""")
                    display_df = df.copy()
                    display_df.columns = [c.replace("_", " ").title() for c in display_df.columns]
                    st.dataframe(display_df, use_container_width=True, hide_index=True)

        # Scenario annotation
        with st.expander("ğŸ“ Add Scenario Note"):
            st.caption(
                "Record assumption overrides, external events, or actuarial judgments "
                "for this segment. Notes are stored in Lakebase (PostgreSQL) and persist across sessions."
            )
            # Pre-populate analyst name from the forwarded user token if available
            _user_token = st.context.headers.get("X-Forwarded-Access-Token", "")
            _default_analyst = _email_from_token(_user_token) if _user_token else ""

            _an_col1, _an_col2 = st.columns(2)
            with _an_col1:
                analyst       = st.text_input("Analyst:", value=_default_analyst)
                scenario_type = st.selectbox("Type:", _SCENARIO_TYPES)
            with _an_col2:
                approval_status = st.selectbox("Status:", _APPROVAL_STATUSES)
                adjustment_pct  = st.number_input(
                    "Forecast adjustment (%):",
                    min_value=-50.0, max_value=50.0, value=0.0, step=0.5,
                    help="Positive = upward revision; negative = downward revision. Enter 0 if no adjustment.",
                )
            note = st.text_area("Notes / assumptions:")
            if st.button("Save Note"):
                adj = adjustment_pct if adjustment_pct != 0.0 else None
                if save_scenario_annotation(selected, note, analyst,
                                            scenario_type, adj, approval_status):
                    st.success("Note saved to Lakebase")

        with st.expander("ğŸ“‹ View Previous Notes"):
            if selected:
                annotations = load_annotations(selected)
                if not annotations.empty:
                    st.dataframe(annotations, use_container_width=True, hide_index=True)
                else:
                    st.info("No annotations yet for this segment.")
    else:
        st.warning("No segments found. The data pipeline may still be running â€” please check back shortly.")

# â”€â”€ Tab 2: Portfolio Risk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab2:
    st.subheader("Monte Carlo Portfolio Risk Summary")
    st.caption("40M baseline paths | t-Copula (df=4) + Lognormal | Property + Auto + Liability")

    with st.expander("â„¹ï¸ How the simulation works", expanded=False):
        st.markdown("""
**Simulation design:**

The portfolio is modelled as three correlated lines of business:
- **Commercial Property** â€” large-loss severity, moderate frequency
- **Commercial Auto** â€” moderate severity, higher frequency
- **Liability** â€” long-tail, low frequency / high severity

Each simulation path draws correlated annual losses using a **Cholesky decomposition**
of the inter-line correlation matrix, applied to independent lognormal samples.
The correlation structure captures co-movement during adverse scenarios (e.g., a
widespread weather event affecting both Property and Auto simultaneously).

**Parameter inputs:**
- Line means and standard deviations are calibrated from the SARIMA forecasts and
  GARCH volatility estimates produced in the modelling pipeline.
- Correlation matrix is assumed (not estimated), with moderate positive correlation (Ï â‰ˆ 0.3â€“0.5)
  between lines â€” conservative for a standard formula approach.

**Dependence model â€” t-Copula (Student-t, df=4):**
Insurance portfolios face *common shocks* â€” catastrophes, judicial inflation, and
macro-economic stress scenarios that affect multiple lines simultaneously. A Gaussian
copula (normal distribution) has *no tail dependence* and **understates VaR** for
these events. The **t-Copula with df=4** captures tail co-movement, producing
higher â€” and more realistic â€” VaR estimates at the 99th and 99.5th percentiles.

**Distributed execution (Module 4):**
The simulation is task-parallelised using **Ray-on-Spark** on GPU workers (`@ray.remote(num_gpus=0.25)`):
100 tasks each generate 1,000 scenarios (100,000 paths total). On serverless runtimes
without Ray, a single-node scipy/NumPy fallback produces equivalent results.

**Output:** 100,000 total portfolio loss values. The empirical distribution of these
values produces the risk metrics shown below.
""")

    summary = load_monte_carlo_summary()

    col1, col2, col3, col4 = st.columns(4)
    col1.metric(
        "Expected Annual Loss",
        f"${summary['expected_loss']:.1f}M",
        help="The mean of the simulated loss distribution â€” the long-run average annual loss across all 100,000 scenarios. This is the actuarial best estimate of what the portfolio will cost in a given year."
    )
    col2.metric(
        "VaR (99%)",
        f"${summary['var_99']:.1f}M",
        help="Value at Risk at the 99th percentile â€” the loss level exceeded in only 1% of scenarios (1-in-100 year event). Under Basel and Solvency frameworks, this is a common risk appetite threshold."
    )
    col3.metric(
        "VaR (99.5%)",
        f"${summary['var_995']:.1f}M",
        help="The Solvency II Solvency Capital Requirement (SCR) calibration point â€” the 1-in-200 year loss. Insurers must hold capital sufficient to cover losses up to this level with 99.5% confidence over a one-year horizon."
    )
    col4.metric(
        "CVaR (99%)",
        f"${summary['cvar_99']:.1f}M",
        help="Conditional Value at Risk (Expected Shortfall) at 99% â€” the average loss across the worst 1% of scenarios. CVaR is more conservative than VaR because it accounts for the severity of tail events, not just the threshold."
    )

    st.divider()

    # Risk metric comparison chart
    metrics = {
        "Expected Loss\n(Mean)": float(summary["expected_loss"]),
        "VaR 99%\n(1-in-100)": float(summary["var_99"]),
        "VaR 99.5%\n(SCR / 1-in-200)": float(summary["var_995"]),
        "CVaR 99%\n(Expected Shortfall)": float(summary["cvar_99"]),
    }
    import plotly.graph_objects as go
    colors = ["#1f77b4", "#ff7f0e", "#d62728", "#9467bd"]
    fig2 = go.Figure(go.Bar(
        x=list(metrics.keys()),
        y=list(metrics.values()),
        marker_color=colors,
        text=[f"${v:.1f}M" for v in metrics.values()],
        textposition="outside",
        hovertemplate="%{x}<br><b>$%{y:.1f}M</b><extra></extra>",
    ))
    fig2.update_layout(
        title="Portfolio Risk Metrics â€” Comparison",
        yaxis_title="Annual Loss ($M)",
        height=380,
        showlegend=False,
        yaxis=dict(range=[0, max(metrics.values()) * 1.25]),
    )
    st.plotly_chart(fig2, use_container_width=True)

    # Simulated distribution using the per-row mean losses as a proxy histogram
    dist_df = load_monte_carlo_distribution()
    if not dist_df.empty:
        for col in dist_df.columns:
            dist_df[col] = pd.to_numeric(dist_df[col], errors='coerce')

        with st.expander("ğŸ“Š Simulated Loss Distribution", expanded=True):
            st.markdown("""
The histogram below shows the distribution of **mean portfolio losses** across simulated scenarios.
The vertical lines mark the key risk thresholds. The heavy right tail is characteristic of
insurance loss distributions â€” most years are near the expected loss, but rare extreme years
can be multiples of the mean.
""")
            fig3 = go.Figure()
            fig3.add_trace(go.Histogram(
                x=dist_df["mean_loss_M"],
                nbinsx=60,
                name="Simulated scenarios",
                marker_color="rgba(31,119,180,0.6)",
                hovertemplate="Loss: $%{x:.1f}M<br>Count: %{y}<extra></extra>",
            ))
            for label, val, color in [
                ("E[Loss]", float(summary["expected_loss"]), "#2ca02c"),
                ("VaR 99%", float(summary["var_99"]), "#ff7f0e"),
                ("VaR 99.5%", float(summary["var_995"]), "#d62728"),
            ]:
                fig3.add_vline(
                    x=val, line_dash="dash", line_color=color,
                    annotation_text=f"{label}: ${val:.1f}M",
                    annotation_position="top right",
                )
            fig3.update_layout(
                xaxis_title="Annual Portfolio Loss ($M)",
                yaxis_title="Number of Scenarios",
                height=360,
                showlegend=False,
            )
            st.plotly_chart(fig3, use_container_width=True)

    st.markdown("""
> **Solvency II context:** The VaR(99.5%) figure is the calibration point for the
> Solvency Capital Requirement (SCR) under the Solvency II Standard Formula. Insurers
> operating under this framework must hold own funds at least equal to their SCR.
> The CVaR (Expected Shortfall) goes further â€” it represents the average cost of the
> scenarios beyond VaR and is increasingly used in IFRS 17 risk adjustment calculations.
""")

# â”€â”€ Tab 3: On-Demand Forecast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab3:
    st.subheader("On-Demand Forecast via Model Serving")
    st.caption("Calls the deployed SARIMA REST endpoint in real time")

    with st.expander("â„¹ï¸ About this endpoint", expanded=False):
        st.markdown("""
**What this does:**

This tab calls a live **Databricks Model Serving** endpoint that wraps the fitted SARIMA model
in a standardised `mlflow.pyfunc` interface. The endpoint is version-controlled â€” it serves
the model tagged as `@Champion` in the Unity Catalog Model Registry.

**Input:** A single integer `horizon` â€” the number of months to forecast ahead.

**Output:** One row per forecast month, with columns:

| Column | Description |
|---|---|
| `month_offset` | Months ahead (1 = next month, 2 = two months ahead, â€¦) |
| `forecast_mean` | Point forecast â€” mean of the predictive distribution |
| `forecast_lo95` | Lower bound of the 95% prediction interval |
| `forecast_hi95` | Upper bound of the 95% prediction interval |

**Why a serving endpoint?**

Deploying the model as a REST endpoint decouples scoring from the training pipeline.
Any downstream system (this app, a pricing tool, a reserving worksheet) can call the
same endpoint without needing access to the training cluster or model code. The
`@Champion` alias means a new model version can be promoted without changing any
calling code.

**Note:** The model is fitted on an aggregate of all segments. For segment-specific
forecasts with the full historical context, use the **Forecasts** tab.
""")

    horizon = st.slider(
        "Forecast horizon (months):",
        min_value=1, max_value=24, value=6,
        help="How many months ahead to forecast. Uncertainty (CI width) grows with horizon."
    )

    if st.button("Generate Forecast"):
        with st.spinner("Calling Model Serving endpoint..."):
            _fetched = call_serving_endpoint(horizon)
            if not _fetched.empty:
                st.session_state["ondemand_result"] = _fetched
                st.session_state["ondemand_horizon"] = horizon
            else:
                st.warning("Endpoint not available â€” start the Model Serving endpoint from Module 5")

    result_df = st.session_state.get("ondemand_result", pd.DataFrame())
    _display_horizon = st.session_state.get("ondemand_horizon", horizon)
    if not result_df.empty:
        # Rename for display
        display_cols = {
            "month_offset": "Month Ahead",
            "forecast_mean": "Point Forecast (mean)",
            "forecast_lo95": "Lower 95% CI",
            "forecast_hi95": "Upper 95% CI",
        }
        display_df = result_df.rename(columns={k: v for k, v in display_cols.items() if k in result_df.columns})

        # Numeric formatting
        for col in display_df.columns:
            if col != "Month Ahead":
                display_df[col] = pd.to_numeric(display_df[col], errors='coerce').round(1)

        st.dataframe(display_df, use_container_width=True, hide_index=True)

        # Visualise the forecast
        if all(c in result_df.columns for c in ["month_offset", "forecast_mean", "forecast_lo95", "forecast_hi95"]):
            result_df["forecast_mean"] = pd.to_numeric(result_df["forecast_mean"], errors='coerce')
            result_df["forecast_lo95"] = pd.to_numeric(result_df["forecast_lo95"], errors='coerce')
            result_df["forecast_hi95"] = pd.to_numeric(result_df["forecast_hi95"], errors='coerce')

            fig4 = go.Figure()
            fig4.add_trace(go.Scatter(
                x=result_df["month_offset"], y=result_df["forecast_mean"],
                mode="lines+markers", name="Point forecast",
                line=dict(color="#FF3419"),
                hovertemplate="Month +%{x}<br>Forecast: %{y:,.1f}<extra></extra>",
            ))
            fig4.add_trace(go.Scatter(
                x=pd.concat([result_df["month_offset"], result_df["month_offset"][::-1]]),
                y=pd.concat([result_df["forecast_hi95"], result_df["forecast_lo95"][::-1]]),
                fill="toself", fillcolor="rgba(255,52,25,0.15)",
                line=dict(color="rgba(255,0,0,0)"),
                name="95% prediction interval",
                hoverinfo="skip",
            ))
            fig4.update_layout(
                title=f"SARIMA On-Demand Forecast â€” {_display_horizon}-Month Horizon",
                xaxis_title="Months Ahead",
                yaxis_title="Forecast Claims Count",
                height=360,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig4, use_container_width=True)

            # Flag widening CI
            ci_width_start = float(result_df["forecast_hi95"].iloc[0] - result_df["forecast_lo95"].iloc[0])
            ci_width_end   = float(result_df["forecast_hi95"].iloc[-1] - result_df["forecast_lo95"].iloc[-1])
            if ci_width_end > ci_width_start * 1.5:
                st.info(
                    f"The 95% interval widens from Â±{ci_width_start/2:,.0f} at month 1 "
                    f"to Â±{ci_width_end/2:,.0f} at month {_display_horizon}, reflecting "
                    "increasing uncertainty at longer horizons â€” expected behaviour for ARIMA models."
                )

        st.download_button(
            "Download CSV",
            result_df.to_csv(index=False),
            file_name=f"sarima_forecast_{_display_horizon}m.csv",
            mime="text/csv",
        )

# â”€â”€ Tab 4: Scenario Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab4:
    st.subheader("On-Demand Monte Carlo Scenario Analysis")
    st.caption("Run custom stress scenarios via the t-Copula Monte Carlo REST endpoint")

    with st.expander("â„¹ï¸ How scenario analysis works", expanded=False):
        st.markdown("""
**What this does:**

This tab calls a live **Monte Carlo Model Serving endpoint** that wraps the t-Copula simulation
(Module 6) as a stateless REST API. Unlike the SARIMA endpoint, all model assumptions are passed
in the request â€” enabling analysts to run stress scenarios without modifying or retraining the model.

**When to use it:**
- **Hard market analysis:** raise means by 15â€“25% to estimate capital impact of deteriorating loss ratios
- **Cat scenario:** increase inter-line correlations (corr_prop_auto â‰¥ 0.6) to simulate a widespread
  catastrophe event affecting Property and Auto simultaneously
- **Parameter uncertainty (ORSA):** increase CVs to represent estimation uncertainty in your loss picks
- **Solvency II sensitivity:** test how VaR(99.5%) changes under different assumption sets

**Input parameters:**

| Group | Parameter | Actuarial meaning |
|---|---|---|
| **Means** | `mean_property_M` / `mean_auto_M` / `mean_liability_M` | Expected annual loss per line ($M) â€” calibrated from pricing or reserving |
| **CVs** | `cv_property` / `cv_auto` / `cv_liability` | Coefficient of variation â€” higher values = fatter tails, wider loss distribution |
| **Correlations** | `corr_prop_auto` / `corr_prop_liab` / `corr_auto_liab` | Inter-line correlation â€” higher values = more capital required for diversification |
| **Simulation** | `n_scenarios`, `copula_df` | Paths (precision vs speed) and t-copula degrees of freedom (lower = heavier tail dependence) |

**Output:** VaR and CVaR risk metrics for the stressed portfolio, compared to the baseline calibration.
""")

    # â”€â”€ Parameter inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### Scenario Parameters")

    col_means, col_cv, col_corr = st.columns(3)

    with col_means:
        st.markdown("**Expected Annual Losses ($M)**")
        mean_prop = st.number_input("Commercial Property", value=12.5, min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $12.5M. Raise to simulate hard market or increased exposure.")
        mean_auto = st.number_input("Commercial Auto",     value=8.3,  min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $8.3M.")
        mean_liab = st.number_input("Liability",           value=5.7,  min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $5.7M. Long-tail; higher means reflect adverse development.")

    with col_cv:
        st.markdown("**Coefficients of Variation**")
        cv_prop = st.slider("Property CV", min_value=0.05, max_value=2.0, value=0.35, step=0.05,
                            help="Baseline: 0.35. Higher CV â†’ fatter loss distribution â†’ higher VaR.")
        cv_auto = st.slider("Auto CV",     min_value=0.05, max_value=2.0, value=0.28, step=0.05,
                            help="Baseline: 0.28.")
        cv_liab = st.slider("Liability CV", min_value=0.05, max_value=2.0, value=0.42, step=0.05,
                            help="Baseline: 0.42. Liability tends to have higher CV due to long-tail uncertainty.")

    with col_corr:
        st.markdown("**Inter-Line Correlations**")
        corr_pa = st.slider("Property â†” Auto",      min_value=0.0, max_value=0.95, value=0.40, step=0.05,
                            help="Baseline: 0.40. Raise to 0.6â€“0.8 for cat scenarios (e.g., widespread storm).")
        corr_pl = st.slider("Property â†” Liability", min_value=0.0, max_value=0.95, value=0.20, step=0.05,
                            help="Baseline: 0.20. Lower â€” these lines are less correlated in normal conditions.")
        corr_al = st.slider("Auto â†” Liability",     min_value=0.0, max_value=0.95, value=0.30, step=0.05,
                            help="Baseline: 0.30.")

    col_sim1, col_sim2 = st.columns(2)
    with col_sim1:
        n_scen = st.select_slider(
            "Number of scenarios",
            options=[1_000, 5_000, 10_000, 25_000, 50_000],
            value=10_000,
            help="More scenarios = more precise estimates but slower (~1â€“5s per 10,000 paths).",
        )
    with col_sim2:
        copula_df_val = st.select_slider(
            "t-Copula degrees of freedom",
            options=[3, 4, 5, 10, 20, 30],
            value=4,
            help="Lower df â†’ heavier tail dependence. df=4 is the actuarial calibration. df=30 â‰ˆ Gaussian copula.",
        )

    # â”€â”€ Baseline reference (read from Delta table, cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _baseline_summary = load_monte_carlo_summary()

    # â”€â”€ Run button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.button("Run Scenario", type="primary"):
        _scenario = {
            "mean_property_M":  mean_prop,
            "mean_auto_M":      mean_auto,
            "mean_liability_M": mean_liab,
            "cv_property":      cv_prop,
            "cv_auto":          cv_auto,
            "cv_liability":     cv_liab,
            "corr_prop_auto":   corr_pa,
            "corr_prop_liab":   corr_pl,
            "corr_auto_liab":   corr_al,
            "n_scenarios":      n_scen,
            "copula_df":        copula_df_val,
        }
        with st.spinner(f"Running Monte Carlo ({n_scen:,} scenarios)..."):
            _fetched = call_monte_carlo_endpoint(_scenario)
        if _fetched:
            st.session_state["scenario_result"] = _fetched
        else:
            st.warning(
                "Monte Carlo endpoint not available. "
                "Start it from Module 6 or wait for the setup job to complete."
            )

    _result = st.session_state.get("scenario_result")
    if _result:
        st.success("Simulation complete")
        st.divider()

        # â”€â”€ Risk metrics comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("#### Results: Scenario vs Baseline")

        _b = _baseline_summary  # Series from load_monte_carlo_summary()

        def _delta(scenario_val, baseline_val):
            if baseline_val and baseline_val != 0:
                d = scenario_val - float(baseline_val)
                return f"{'+' if d >= 0 else ''}${d:.1f}M"
            return None

        rc1, rc2, rc3, rc4 = st.columns(4)
        rc1.metric(
            "Expected Loss",
            f"${_result['expected_loss_M']:.1f}M",
            delta=_delta(_result['expected_loss_M'], _b.get('expected_loss', 0)),
            help="Mean portfolio loss across all simulated scenarios.",
        )
        rc2.metric(
            "VaR (99%)",
            f"${_result['var_99_M']:.1f}M",
            delta=_delta(_result['var_99_M'], _b.get('var_99', 0)),
            help="1-in-100 year loss level.",
        )
        rc3.metric(
            "VaR (99.5%) â€” SCR",
            f"${_result['var_995_M']:.1f}M",
            delta=_delta(_result['var_995_M'], _b.get('var_995', 0)),
            help="Solvency II SCR calibration point (1-in-200 year).",
        )
        rc4.metric(
            "CVaR (99%)",
            f"${_result['cvar_99_M']:.1f}M",
            delta=_delta(_result['cvar_99_M'], _b.get('cvar_99', 0)),
            help="Expected loss in the worst 1% of scenarios.",
        )

        # â”€â”€ Waterfall chart: Scenario vs Baseline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import plotly.graph_objects as go

        _metrics_labels = ["E[Loss]", "VaR 95%", "VaR 99%", "VaR 99.5%\n(SCR)", "CVaR 99%"]
        _baseline_vals = [
            float(_b.get("expected_loss", 0)),
            float(_b.get("var_99", 0)) * 0.85,   # approximate VaR95 from VaR99
            float(_b.get("var_99", 0)),
            float(_b.get("var_995", 0)),
            float(_b.get("cvar_99", 0)),
        ]
        _scenario_vals = [
            _result["expected_loss_M"],
            _result["var_95_M"],
            _result["var_99_M"],
            _result["var_995_M"],
            _result["cvar_99_M"],
        ]

        _fig_cmp = go.Figure()
        _fig_cmp.add_trace(go.Bar(
            name="Baseline (pre-computed)",
            x=_metrics_labels,
            y=_baseline_vals,
            marker_color="rgba(31,119,180,0.7)",
            hovertemplate="%{x}<br>Baseline: $%{y:.1f}M<extra></extra>",
        ))
        _fig_cmp.add_trace(go.Bar(
            name="Scenario (on-demand)",
            x=_metrics_labels,
            y=_scenario_vals,
            marker_color="rgba(214,39,40,0.7)",
            hovertemplate="%{x}<br>Scenario: $%{y:.1f}M<extra></extra>",
        ))
        _fig_cmp.update_layout(
            title="Scenario vs Baseline Risk Metrics",
            yaxis_title="Annual Portfolio Loss ($M)",
            barmode="group",
            height=380,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        )
        st.plotly_chart(_fig_cmp, use_container_width=True)

        # â”€â”€ Raw metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.expander("ğŸ“‹ Full scenario metrics"):
            _display = {
                "Metric": ["Expected Loss", "VaR (95%)", "VaR (99%)", "VaR (99.5% SCR)", "CVaR (99%)", "Max Loss"],
                "Scenario ($M)": [
                    f"${_result['expected_loss_M']:.2f}",
                    f"${_result['var_95_M']:.2f}",
                    f"${_result['var_99_M']:.2f}",
                    f"${_result['var_995_M']:.2f}",
                    f"${_result['cvar_99_M']:.2f}",
                    f"${_result['max_loss_M']:.2f}",
                ],
                "Copula": [_result.get("copula", ""), "", "", "", "", ""],
                "Scenarios": [f"{int(_result.get('n_scenarios_used', n_scen)):,}", "", "", "", "", ""],
            }
            st.dataframe(pd.DataFrame(_display), use_container_width=True, hide_index=True)

# â”€â”€ Tab 5: Catastrophe Scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab5:
    st.subheader("Catastrophe Scenario Analysis")
    st.caption(
        "Pre-computed stress tests (Module 4) + custom catastrophe event simulation via Monte Carlo endpoint"
    )

    import plotly.graph_objects as go

    # â”€â”€ Section 1: Pre-computed stress scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### Pre-Computed Stress Test Results")
    st.caption(
        "Run by Module 4 (Ray + GPU) at deploy time â€” 120M paths across 3 scenarios."
    )

    _baseline_smry = load_monte_carlo_summary()
    _stress_df     = load_stress_scenarios()

    if not _stress_df.empty:
        _b_var995  = float(_baseline_smry.get('var_995', 0))
        _b_var99   = float(_baseline_smry.get('var_99', 0))
        _b_cvar99  = float(_baseline_smry.get('cvar_99', 0))
        _b_mean    = float(_baseline_smry.get('expected_loss', 0))

        _disp_rows = [{"Scenario": "Baseline", "Exp. Loss": f"${_b_mean:.1f}M",
                       "VaR(99%)": f"${_b_var99:.1f}M", "VaR(99.5%) SCR": f"${_b_var995:.1f}M",
                       "CVaR(99%)": f"${_b_cvar99:.1f}M", "Î” VaR(99.5%)": "â€”"}]
        for _, row in _stress_df.iterrows():
            _disp_rows.append({
                "Scenario":       row["scenario_label"],
                "Exp. Loss":      f"${row['total_mean_M']:.1f}M",
                "VaR(99%)":       f"${row['var_99_M']:.1f}M",
                "VaR(99.5%) SCR": f"${row['var_995_M']:.1f}M",
                "CVaR(99%)":      f"${row['cvar_99_M']:.1f}M",
                "Î” VaR(99.5%)":   f"{row['var_995_vs_baseline']:+.1f}%",
            })
        st.dataframe(pd.DataFrame(_disp_rows), use_container_width=True, hide_index=True)

        _all_labels = ["Baseline"] + _stress_df["scenario_label"].tolist()
        _all_var995 = [_b_var995] + _stress_df["var_995_M"].tolist()
        _all_cvar99 = [_b_cvar99] + _stress_df["cvar_99_M"].tolist()
        _bar_colors = ["#1f77b4", "#ff7f0e", "#d62728", "#9467bd"]
        _bar_colors_muted = [
            f"rgba({int(c[1:3],16)},{int(c[3:5],16)},{int(c[5:7],16)},0.5)"
            for c in _bar_colors
        ]

        _fig_stress = go.Figure()
        _fig_stress.add_trace(go.Bar(
            name="VaR(99.5%) â€” SCR",
            x=_all_labels, y=_all_var995,
            marker_color=_bar_colors,
            text=[f"${v:.1f}M" for v in _all_var995], textposition="outside",
            hovertemplate="%{x}<br>VaR(99.5%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_stress.add_trace(go.Bar(
            name="CVaR(99%)",
            x=_all_labels, y=_all_cvar99,
            marker_color=_bar_colors_muted,
            text=[f"${v:.1f}M" for v in _all_cvar99], textposition="outside",
            hovertemplate="%{x}<br>CVaR(99%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_stress.update_layout(
            title="VaR(99.5%) and CVaR(99%) â€” Baseline vs. Stress Scenarios",
            yaxis_title="Annual Portfolio Loss ($M)",
            barmode="group", height=400, showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            yaxis=dict(range=[0, max(_all_var995 + _all_cvar99) * 1.25]),
        )
        st.plotly_chart(_fig_stress, use_container_width=True)
    else:
        st.info("Stress scenario data not yet available â€” run the setup job (e2-demo-ray target).")

    # â”€â”€ Section 2: VaR evolution timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### 12-Month Forward VaR Evolution (SARIMA-Driven)")
    _timeline_df = load_var_timeline()
    if not _timeline_df.empty:
        _fig_tl = go.Figure()
        _fig_tl.add_trace(go.Scatter(
            x=_timeline_df["month_idx"], y=_timeline_df["var_995_M"],
            mode="lines+markers", name="VaR(99.5%)", line=dict(color="#d62728"),
            hovertemplate="Month +%{x}<br>VaR(99.5%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_tl.add_trace(go.Scatter(
            x=_timeline_df["month_idx"], y=_timeline_df["var_99_M"],
            mode="lines+markers", name="VaR(99%)", line=dict(color="#ff7f0e", dash="dash"),
            hovertemplate="Month +%{x}<br>VaR(99%): $%{y:.1f}M<extra></extra>",
        ))
        _b_v995 = float(_baseline_smry.get('var_995', 0))
        if _b_v995 > 0:
            _fig_tl.add_hline(
                y=_b_v995, line_dash="dot", line_color="grey",
                annotation_text=f"Current VaR(99.5%): ${_b_v995:.1f}M",
                annotation_position="bottom right",
            )
        _fig_tl.update_layout(
            title="Forward Capital Requirement Curve â€” VaR Evolution Along SARIMA Forecast Path",
            xaxis_title="Months Ahead", yaxis_title="Annual Portfolio Loss ($M)",
            height=380,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        )
        st.plotly_chart(_fig_tl, use_container_width=True)
    else:
        st.info("VaR timeline not yet available â€” run the setup job (e2-demo-ray target).")

    st.divider()

    # â”€â”€ Section 3: Custom CAT scenario submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### Submit Custom Catastrophe Scenario")
    st.caption(
        "Configure a natural disaster or market event, translate it to portfolio loss parameters, "
        "and run a Monte Carlo simulation via the serving endpoint. Results are saved to Lakebase."
    )

    _cat_top1, _cat_top2 = st.columns(2)
    with _cat_top1:
        _cat_type = st.selectbox(
            "Event type:",
            list(_CAT_PRESETS.keys()),
            help="Preset determines default severity multipliers for each line of business.",
        )
        st.caption(f"_{_CAT_PRESETS[_cat_type]['desc']}_")
        _return_period = st.selectbox(
            "Return period:",
            list(_RETURN_PERIOD_MULT.keys()),
            index=1,
            help="1-in-250yr â‰ˆ Solvency II PML benchmark; 1-in-500yr = extreme stress.",
        )
    with _cat_top2:
        _affected_regions = st.multiselect(
            "Affected provinces/territories:",
            _CANADIAN_PROVINCES,
            default=["Ontario", "Quebec"],
            help="Provinces exposed to this event (for documentation and audit trail).",
        )
        _affected_lines = st.multiselect(
            "Affected product lines:",
            ["Commercial Property", "Commercial Auto", "Personal Auto", "Homeowners", "Liability"],
            default=["Commercial Property", "Commercial Auto"],
            help="Product lines most directly impacted (for documentation).",
        )

    # Compute default params from preset + return period
    _preset   = _CAT_PRESETS[_cat_type]
    _rp_mult  = _RETURN_PERIOD_MULT[_return_period]
    _bm, _bc, _bco = [12.5, 8.3, 5.7], [0.35, 0.28, 0.42], [0.40, 0.20, 0.30]
    _def_means = [round(m * mu * _rp_mult, 1) for m, mu in zip(_bm, _preset["means_mult"])]
    _def_cvs   = [round(c * cu, 2) for c, cu in zip(_bc, _preset["cv_mult"])]
    _def_corrs = [min(c + _preset["corr_add"], 0.95) for c in _bco]

    with st.expander("âš™ï¸ Adjust scenario parameters", expanded=False):
        _adj1, _adj2, _adj3 = st.columns(3)
        with _adj1:
            st.markdown("**Expected Annual Losses ($M)**")
            _cat_mean_prop = st.number_input("Property", value=_def_means[0], min_value=0.1, max_value=2000.0, step=1.0, key="cat_mp")
            _cat_mean_auto = st.number_input("Auto",     value=_def_means[1], min_value=0.1, max_value=2000.0, step=1.0, key="cat_ma")
            _cat_mean_liab = st.number_input("Liability",value=_def_means[2], min_value=0.1, max_value=2000.0, step=1.0, key="cat_ml")
        with _adj2:
            st.markdown("**Coefficients of Variation**")
            _cat_cv_prop = st.slider("Property CV", 0.05, 3.0, _def_cvs[0], 0.05, key="cat_cvp")
            _cat_cv_auto = st.slider("Auto CV",     0.05, 3.0, _def_cvs[1], 0.05, key="cat_cva")
            _cat_cv_liab = st.slider("Liability CV",0.05, 3.0, _def_cvs[2], 0.05, key="cat_cvl")
        with _adj3:
            st.markdown("**Inter-Line Correlations**")
            _cat_corr_pa = st.slider("Property â†” Auto",      0.0, 0.95, _def_corrs[0], 0.05, key="cat_cpa")
            _cat_corr_pl = st.slider("Property â†” Liability", 0.0, 0.95, _def_corrs[1], 0.05, key="cat_cpl")
            _cat_corr_al = st.slider("Auto â†” Liability",     0.0, 0.95, _def_corrs[2], 0.05, key="cat_cal")
    _cat_n_scen = st.select_slider(
        "Simulation paths:",
        options=[5_000, 10_000, 25_000, 50_000],
        value=25_000,
        help="25,000 recommended for CAT scenarios â€” balances precision and response time.",
        key="cat_nscen",
    )

    _user_tok5 = st.context.headers.get("X-Forwarded-Access-Token", "")
    _cat_analyst = _email_from_token(_user_tok5) if _user_tok5 else ""
    _cat_analyst_in = st.text_input("Analyst:", value=_cat_analyst, key="cat_analyst")
    _cat_note_in    = st.text_area(
        "Scenario rationale / assumptions:",
        placeholder=f"Describe the {_cat_type} scenario, basis for severity assumptions, affected exposure...",
        key="cat_note",
    )

    if st.button("ğŸŒªï¸ Run CAT Scenario", type="primary"):
        _cat_scenario_params = {
            "mean_property_M":  _cat_mean_prop,
            "mean_auto_M":      _cat_mean_auto,
            "mean_liability_M": _cat_mean_liab,
            "cv_property":      _cat_cv_prop,
            "cv_auto":          _cat_cv_auto,
            "cv_liability":     _cat_cv_liab,
            "corr_prop_auto":   _cat_corr_pa,
            "corr_prop_liab":   _cat_corr_pl,
            "corr_auto_liab":   _cat_corr_al,
            "n_scenarios":      _cat_n_scen,
            "copula_df":        4,
        }
        with st.spinner(f"Running {_cat_type} ({_return_period}) â€” {_cat_n_scen:,} scenarios..."):
            _cat_result = call_monte_carlo_endpoint(_cat_scenario_params)

        if _cat_result:
            st.success(f"âœ… {_cat_type} ({_return_period}) complete")

            _b5 = _baseline_smry

            def _cat_delta(sc_val, base_key):
                bv = float(_b5.get(base_key, 0))
                d  = sc_val - bv
                return f"{'+' if d >= 0 else ''}${d:.1f}M"

            _cr1, _cr2, _cr3, _cr4 = st.columns(4)
            _cr1.metric("Expected Loss",     f"${_cat_result['expected_loss_M']:.1f}M",
                        delta=_cat_delta(_cat_result['expected_loss_M'], 'expected_loss'))
            _cr2.metric("VaR(99%)",          f"${_cat_result['var_99_M']:.1f}M",
                        delta=_cat_delta(_cat_result['var_99_M'], 'var_99'))
            _cr3.metric("VaR(99.5%) â€” SCR",  f"${_cat_result['var_995_M']:.1f}M",
                        delta=_cat_delta(_cat_result['var_995_M'], 'var_995'))
            _cr4.metric("CVaR(99%)",         f"${_cat_result['cvar_99_M']:.1f}M",
                        delta=_cat_delta(_cat_result['cvar_99_M'], 'cvar_99'))

            # Save annotation to Lakebase
            _var_lift = ((_cat_result['var_995_M'] / max(float(_b5.get('var_995', 1)), 0.01)) - 1.0) * 100
            _full_note = (
                f"[{_cat_type} | {_return_period}] "
                f"Regions: {', '.join(_affected_regions) if _affected_regions else 'N/A'} | "
                f"Lines: {', '.join(_affected_lines) if _affected_lines else 'N/A'} | "
                f"Prop ${_cat_mean_prop:.1f}M/CV={_cat_cv_prop:.2f}, "
                f"Auto ${_cat_mean_auto:.1f}M/CV={_cat_cv_auto:.2f}, "
                f"Liab ${_cat_mean_liab:.1f}M/CV={_cat_cv_liab:.2f} | "
                f"VaR(99.5%)=${_cat_result['var_995_M']:.1f}M, CVaR=${_cat_result['cvar_99_M']:.1f}M"
                + (f" | {_cat_note_in}" if _cat_note_in else "")
            )
            if save_scenario_annotation(
                segment_id="CAT_SCENARIO",
                note=_full_note,
                analyst=_cat_analyst_in,
                scenario_type=f"Catastrophe: {_cat_type}",
                adjustment_pct=round(_var_lift, 1),
                approval_status="Draft",
            ):
                st.caption("âœ“ Scenario saved to Lakebase audit log")
        else:
            st.warning(
                "Monte Carlo endpoint not available. "
                "Start it from Module 6 or wait for the setup job to complete."
            )

    # Recent CAT scenarios
    st.divider()
    st.markdown("### Recent CAT Scenario Audit Log (Lakebase)")
    _cat_history = load_annotations("CAT_SCENARIO")
    if not _cat_history.empty:
        st.dataframe(_cat_history, use_container_width=True, hide_index=True)
    else:
        st.info("No catastrophe scenarios submitted yet. Use the form above to run one.")
