import os
import base64
import streamlit as st
import pandas as pd
import numpy as np
import requests
import json

# â”€â”€â”€ Lazy import for psycopg2 (may not be available in all environments) â”€â”€â”€â”€â”€
_psycopg2 = None
_psycopg2_error = None

def _get_psycopg2():
    """Lazy-load psycopg2 on first use; cache the result."""
    global _psycopg2, _psycopg2_error
    if _psycopg2 is not None:
        return _psycopg2
    if _psycopg2_error is not None:
        return None
    try:
        import psycopg2 as _pg
        _psycopg2 = _pg
        return _psycopg2
    except ImportError as exc:
        _psycopg2_error = str(exc)
        return None

# â”€â”€â”€ Page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="Insurance Portfolio Risk Intelligence",
    page_icon="ğŸ“Š",
    layout="wide",
)

# â”€â”€â”€ Authentication â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_workspace_client = None
_auth_init_error = None

def _get_workspace_client():
    """Lazily initialise the Databricks WorkspaceClient."""
    global _workspace_client, _auth_init_error
    if _workspace_client is not None:
        return _workspace_client
    if _auth_init_error is not None:
        return None
    try:
        from databricks import sdk
        _workspace_client = sdk.WorkspaceClient()
        return _workspace_client
    except Exception as exc:
        _auth_init_error = str(exc)
        return None

def get_token():
    """Get OAuth token for user-context operations (Lakebase, model serving).
    Prefers the forwarded user token, falls back to the SDK service principal token."""
    user_token = st.context.headers.get("X-Forwarded-Access-Token")
    if user_token:
        return user_token
    w = _get_workspace_client()
    if w is not None:
        try:
            if w.config.token:
                return w.config.token
            headers = {}
            w.config.authenticate(headers)
            if "Authorization" in headers:
                return headers["Authorization"].replace("Bearer ", "")
        except Exception:
            pass
    return None

# â”€â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
WORKSPACE_HOST = os.environ.get("DATABRICKS_HOST", "")
if WORKSPACE_HOST and not WORKSPACE_HOST.startswith("http"):
    WORKSPACE_HOST = f"https://{WORKSPACE_HOST}"

WAREHOUSE_ID  = os.environ.get("DATABRICKS_WAREHOUSE_ID", "")

# CATALOG / SCHEMA / PG_DATABASE / ENDPOINT_NAME / LAKEBASE_ENDPOINT_PATH come
# from app/_bundle_config.py, generated by deploy.sh before each deploy.
try:
    from _bundle_config import (
        CATALOG as _BC_CATALOG,
        SCHEMA as _BC_SCHEMA,
        PG_DATABASE as _BC_PG,
        ENDPOINT_NAME as _BC_ENDPOINT,
        LAKEBASE_ENDPOINT_PATH as _BC_LAKEBASE_PATH,
        LAKEBASE_HOST as _BC_LAKEBASE_HOST,
    )
    try:
        from _bundle_config import MC_ENDPOINT_NAME as _BC_MC_ENDPOINT
    except ImportError:
        _BC_MC_ENDPOINT = "actuarial-workshop-monte-carlo"
except ImportError:
    _BC_CATALOG = "my_catalog"
    _BC_SCHEMA = "actuarial_workshop"
    _BC_PG = "actuarial_workshop_db"
    _BC_ENDPOINT = "actuarial-workshop-sarima-forecaster"
    _BC_MC_ENDPOINT = "actuarial-workshop-monte-carlo"
    _BC_LAKEBASE_PATH = "projects/actuarial-workshop-lakebase/branches/main/endpoints/primary"
    _BC_LAKEBASE_HOST = ""

CATALOG                = os.environ.get("CATALOG") or _BC_CATALOG
SCHEMA                 = os.environ.get("SCHEMA") or _BC_SCHEMA
PG_DATABASE_DEFAULT    = _BC_PG
ENDPOINT_NAME          = os.environ.get("ENDPOINT_NAME") or _BC_ENDPOINT
MC_ENDPOINT_NAME       = os.environ.get("MC_ENDPOINT_NAME") or _BC_MC_ENDPOINT
LAKEBASE_ENDPOINT_PATH = os.environ.get("LAKEBASE_ENDPOINT_PATH") or _BC_LAKEBASE_PATH

if _auth_init_error is not None:
    st.warning(
        f"Databricks SDK could not initialise: {_auth_init_error}. "
        "Data features will be unavailable until the issue is resolved."
    )

# â”€â”€â”€ SQL Execution Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def execute_sql(statement: str) -> pd.DataFrame:
    """Execute SQL via the Databricks SDK StatementExecution â€” handles auth automatically."""
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return pd.DataFrame()
    try:
        result = w.statement_execution.execute_statement(
            warehouse_id=WAREHOUSE_ID,
            statement=statement,
            wait_timeout="30s",
        )
        state = result.status.state.value if result.status and result.status.state else "UNKNOWN"
        if state == "SUCCEEDED":
            columns = []
            if result.manifest and result.manifest.schema and result.manifest.schema.columns:
                columns = [c.name for c in result.manifest.schema.columns]
            rows = []
            if result.result and result.result.data_array:
                rows = result.result.data_array
            if columns and rows:
                return pd.DataFrame(rows, columns=columns)
            return pd.DataFrame()
        else:
            error_msg = "Unknown SQL error"
            if result.status and result.status.error:
                error_msg = result.status.error.message or error_msg
            st.error(f"SQL Error ({state}): {error_msg}")
            return pd.DataFrame()
    except Exception as exc:
        st.error(f"SQL execution failed: {exc}")
        return pd.DataFrame()

# â”€â”€â”€ Data Loading Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@st.cache_data(ttl=300)
def load_segments():
    df = execute_sql(f"SELECT DISTINCT segment_id FROM {CATALOG}.{SCHEMA}.sarima_forecasts ORDER BY 1")
    if not df.empty:
        return df["segment_id"].tolist()
    return []

@st.cache_data(ttl=300)
def load_forecasts(segment_id: str):
    return execute_sql(f"""
        SELECT month, record_type, claims_count, forecast_mean, forecast_lo95, forecast_hi95
        FROM {CATALOG}.{SCHEMA}.sarima_forecasts
        WHERE segment_id = '{segment_id}'
        ORDER BY month
    """)

@st.cache_data(ttl=600)
def load_monte_carlo_summary():
    df = execute_sql(f"""
        SELECT
            AVG(mean_loss_M)  AS expected_loss,
            AVG(var_99_M)     AS var_99,
            AVG(var_995_M)    AS var_995,
            AVG(cvar_99_M)    AS cvar_99,
            MAX(max_loss_M)   AS max_loss
        FROM {CATALOG}.{SCHEMA}.monte_carlo_results
    """)
    if not df.empty:
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df.iloc[0]
    return pd.Series({
        'expected_loss': 0, 'var_99': 0, 'var_995': 0, 'cvar_99': 0, 'max_loss': 0
    })

@st.cache_data(ttl=600)
def load_monte_carlo_distribution():
    """Load per-simulation total loss for portfolio loss distribution chart."""
    return execute_sql(f"""
        SELECT mean_loss_M, var_99_M, var_995_M, cvar_99_M, max_loss_M
        FROM {CATALOG}.{SCHEMA}.monte_carlo_results
        ORDER BY mean_loss_M
    """)

@st.cache_data(ttl=600)
def load_segment_stats(segment_id: str):
    """Summary statistics for the selected segment's history."""
    return execute_sql(f"""
        SELECT
            MIN(month)                          AS first_month,
            MAX(month)                          AS last_month,
            COUNT(*)                            AS num_months,
            ROUND(AVG(claims_count), 1)         AS avg_monthly_claims,
            ROUND(STDDEV(claims_count), 1)      AS stddev_claims,
            MIN(claims_count)                   AS min_claims,
            MAX(claims_count)                   AS max_claims
        FROM {CATALOG}.{SCHEMA}.gold_claims_monthly
        WHERE segment_id = '{segment_id}'
    """)

@st.cache_data(ttl=600)
def load_stress_scenarios():
    """Load pre-computed stress test scenario comparison from Module 4."""
    df = execute_sql(f"""
        SELECT scenario_label, total_mean_M, var_99_M, var_995_M, cvar_99_M, var_995_vs_baseline
        FROM {CATALOG}.{SCHEMA}.stress_test_scenarios
        ORDER BY var_995_M DESC
    """)
    if not df.empty:
        for col in ['total_mean_M', 'var_99_M', 'var_995_M', 'cvar_99_M', 'var_995_vs_baseline']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

@st.cache_data(ttl=600)
def load_var_timeline():
    """Load the 12-month SARIMA-driven VaR evolution from Module 4."""
    df = execute_sql(f"""
        SELECT forecast_month, month_idx, total_mean_M, var_99_M, var_995_M, cvar_99_M, var_995_vs_baseline
        FROM {CATALOG}.{SCHEMA}.portfolio_risk_timeline
        ORDER BY month_idx
    """)
    if not df.empty:
        for col in ['total_mean_M', 'var_99_M', 'var_995_M', 'cvar_99_M', 'var_995_vs_baseline', 'month_idx']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def load_garch_volatility(segment_id: str):
    """Load GARCH conditional volatility for a segment."""
    df = execute_sql(f"""
        SELECT month, loss_ratio, cond_volatility, forecast_vol_1m, forecast_vol_12m
        FROM {CATALOG}.{SCHEMA}.garch_volatility
        WHERE segment_id = '{segment_id}' AND record_type = 'actual'
        ORDER BY month
    """)
    if not df.empty:
        for col in ['loss_ratio', 'cond_volatility', 'forecast_vol_1m', 'forecast_vol_12m']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def load_regional_forecast():
    """Load regional claims forecast from Module 4."""
    df = execute_sql(f"""
        SELECT region, forecast_month, total_forecast_claims
        FROM {CATALOG}.{SCHEMA}.regional_claims_forecast
        ORDER BY region, forecast_month
    """)
    if not df.empty:
        df['total_forecast_claims'] = pd.to_numeric(df['total_forecast_claims'], errors='coerce')
    return df

def load_reserve_triangle():
    """Load the loss development triangle from the DLT pipeline."""
    df = execute_sql(f"""
        SELECT segment_id, product_line, region, accident_month, dev_lag,
               cumulative_paid, cumulative_incurred, case_reserve
        FROM {CATALOG}.{SCHEMA}.gold_reserve_triangle
        ORDER BY segment_id, accident_month, dev_lag
    """)
    if not df.empty:
        for col in ['cumulative_paid', 'cumulative_incurred', 'case_reserve', 'dev_lag']:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def call_serving_endpoint(horizon: int) -> pd.DataFrame:
    """Call SARIMA Model Serving endpoint via Databricks SDK."""
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return pd.DataFrame()
    try:
        response = w.serving_endpoints.query(
            name=ENDPOINT_NAME,
            dataframe_records=[{"horizon": horizon}],
        )
        predictions = response.predictions
        if predictions:
            return pd.DataFrame(predictions)
        return pd.DataFrame()
    except Exception as exc:
        st.warning(f"Endpoint unavailable: {exc}")
        return pd.DataFrame()

def call_monte_carlo_endpoint(scenario: dict) -> dict | None:
    """Call the Monte Carlo serving endpoint with a scenario parameter dict.

    Returns a dict of risk metrics or None on error.
    """
    w = _get_workspace_client()
    if w is None:
        st.error(f"Databricks SDK unavailable: {_auth_init_error or 'unknown error'}")
        return None
    try:
        response = w.serving_endpoints.query(
            name=MC_ENDPOINT_NAME,
            dataframe_records=[scenario],
        )
        predictions = response.predictions
        if predictions:
            p = predictions[0] if isinstance(predictions, list) else predictions
            return {k: float(v) if k != "copula" else v for k, v in p.items()}
        return None
    except Exception as exc:
        st.warning(f"Monte Carlo endpoint unavailable: {exc}")
        return None

# â”€â”€â”€ Lakebase: Scenario annotations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _email_from_token(token: str) -> str:
    """Decode the JWT payload (without verification) to extract the user identity."""
    try:
        payload_b64 = token.split(".")[1]
        payload_b64 += "=" * (4 - len(payload_b64) % 4)
        payload = json.loads(base64.urlsafe_b64decode(payload_b64))
        return payload.get("sub", payload.get("email", ""))
    except Exception:
        return ""

_lakebase_cred_cache: dict = {"token": None, "expires_at": 0.0}

def _get_lakebase_host() -> str:
    """Return the Lakebase endpoint hostname.

    PGHOST env var takes precedence (can be overridden at runtime).
    Falls back to LAKEBASE_HOST from _bundle_config.py, which is written by
    deploy.sh after lakebase_setup.py resolves and confirms the hostname.
    """
    return os.environ.get("PGHOST") or _BC_LAKEBASE_HOST

def _get_lakebase_credential() -> tuple:
    """Return (client_id, token) for the app SP via generate_database_credential.

    Uses w.postgres.generate_database_credential() (Databricks SDK >= 0.81.0).
    The token is used as the Postgres password â€” Lakebase Autoscaling validates it
    via the databricks_auth extension. Credentials are cached until 60 s before expiry.

    DATABRICKS_CLIENT_ID is injected at runtime by the Databricks Apps platform.
    """
    import time
    now = time.time()
    client_id = os.environ.get("DATABRICKS_CLIENT_ID", "")
    if _lakebase_cred_cache["token"] and _lakebase_cred_cache["expires_at"] > now + 60:
        return client_id, _lakebase_cred_cache["token"]

    w = _get_workspace_client()
    if w is None:
        return client_id, None
    try:
        # SDK >= 0.81.0: w.postgres.generate_database_credential(endpoint=<resource_path>)
        cred = w.postgres.generate_database_credential(endpoint=LAKEBASE_ENDPOINT_PATH)
        token = cred.token
    except AttributeError:
        # Fallback: low-level API call (older SDK versions without w.postgres)
        try:
            cred = w.api_client.do(
                "POST",
                "/api/2.0/postgres/generateDatabaseCredential",
                body={"endpoint": LAKEBASE_ENDPOINT_PATH},
            )
            token = cred.get("token", "") if isinstance(cred, dict) else ""
        except Exception:
            return client_id, None
    except Exception:
        return client_id, None
    # Credentials typically expire in 1 hour; cache conservatively
    _lakebase_cred_cache["token"] = token
    _lakebase_cred_cache["expires_at"] = now + 3300  # 55 min
    return client_id, token


def get_lakebase_conn():
    """Connect to the Lakebase Autoscaling endpoint as the app service principal.

    Authentication uses generate_database_credential â€” the returned token is used
    directly as the Postgres password. The SP role is created by databricks_create_role()
    in app_setup.py (Task 7 of the setup job).

    All DB operations run as the SP. The human analyst's identity is captured from
    the forwarded user token and stored in the `analyst` column.
    """
    psycopg2 = _get_psycopg2()
    if psycopg2 is None:
        raise RuntimeError(
            f"psycopg2 is not available ({_psycopg2_error or 'unknown reason'}). "
            "Lakebase features are disabled."
        )

    host = _get_lakebase_host()
    if not host:
        raise RuntimeError(
            "Lakebase hostname not configured. "
            "Re-run deploy.sh to regenerate app/_bundle_config.py with LAKEBASE_HOST, "
            "or set the PGHOST environment variable."
        )

    sp_user, sp_token = _get_lakebase_credential()
    if not sp_token:
        raise RuntimeError("Could not obtain Lakebase credential â€” check DATABRICKS_CLIENT_ID env var.")

    port     = int(os.environ.get("PGPORT", "5432"))
    database = PG_DATABASE_DEFAULT
    sslmode  = os.environ.get("PGSSLMODE", "require")
    return psycopg2.connect(
        host=host, port=port, database=database,
        user=sp_user, password=sp_token, sslmode=sslmode,
    )

_annotations_table_ensured = False

def _ensure_annotations_table():
    """Create scenario_annotations if it doesn't exist (idempotent, runs once per process)."""
    global _annotations_table_ensured
    if _annotations_table_ensured:
        return
    try:
        conn = get_lakebase_conn()
        cur = conn.cursor()
        # Explicit public schema â€” Lakebase sets search_path = "$user", public so an
        # unqualified name would land in the connecting user's personal schema instead.
        cur.execute("""
            CREATE TABLE IF NOT EXISTS public.scenario_annotations (
                id              SERIAL        PRIMARY KEY,
                segment_id      TEXT          NOT NULL,
                note            TEXT,
                analyst         TEXT,
                scenario_type   TEXT,
                adjustment_pct  NUMERIC(10,2),
                approval_status TEXT          DEFAULT 'Draft',
                created_at      TIMESTAMP     DEFAULT NOW()
            )
        """)
        conn.commit()
        conn.close()
        _annotations_table_ensured = True
    except Exception:
        pass  # Non-fatal â€” table may already exist or Lakebase may be unavailable

_SCENARIO_TYPES = [
    "Assumption Override",
    "Catastrophe Event",
    "External Event",
    "Review Comment",
    "Judgment Adjustment",
    "Model Calibration",
]

_APPROVAL_STATUSES = ["Draft", "Pending Review", "Approved"]

# â”€â”€â”€ CAT scenario presets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Each preset defines severity multipliers (relative to baseline) applied to
# means, CVs, and correlation offsets.  Return period further scales means.
_CAT_PRESETS = {
    "Hurricane":           {
        "means_mult": [2.8, 1.9, 1.3], "cv_mult": [1.4, 1.3, 1.2], "corr_add": 0.15,
        "desc": "Severe wind + storm surge. Property and Auto heavily impacted.",
    },
    "Earthquake":          {
        "means_mult": [3.8, 1.6, 1.4], "cv_mult": [1.6, 1.2, 1.3], "corr_add": 0.20,
        "desc": "Structural damage + fires. Property losses dominate.",
    },
    "Flood":               {
        "means_mult": [2.3, 1.7, 1.1], "cv_mult": [1.3, 1.2, 1.1], "corr_add": 0.12,
        "desc": "Widespread inundation. Both Property and Auto impacted.",
    },
    "Wildfire":            {
        "means_mult": [3.2, 1.4, 1.2], "cv_mult": [1.5, 1.1, 1.2], "corr_add": 0.15,
        "desc": "Structure and vehicle losses across a broad geographic area.",
    },
    "Pandemic":            {
        "means_mult": [1.1, 0.8, 2.8], "cv_mult": [1.2, 1.1, 1.5], "corr_add": 0.25,
        "desc": "Liability and healthcare claims surge; Auto frequency drops.",
    },
    "Market Crash":        {
        "means_mult": [1.2, 1.1, 1.8], "cv_mult": [1.3, 1.2, 1.4], "corr_add": 0.30,
        "desc": "Systemic financial stress â€” correlated across all lines.",
    },
    "Industrial Accident": {
        "means_mult": [2.0, 1.5, 2.2], "cv_mult": [1.3, 1.2, 1.4], "corr_add": 0.18,
        "desc": "Explosion, chemical spill, or structural collapse. High liability.",
    },
    "Zombies": {
        "means_mult": [144.0, 217.0, 316.0], "cv_mult": [7.71, 9.64, 6.43], "corr_add": 0.50,
        "desc": "Societal collapse â€” 90% of maximum possible loss across all lines.",
    },
    "Asteroid": {
        "means_mult": [200.0, 300.0, 500.0], "cv_mult": [9.0, 11.0, 8.0], "corr_add": 1.0,
        "desc": "Extinction-level event â€” total portfolio loss (100% of maximum) across all lines.",
    },
}

_RETURN_PERIOD_MULT = {
    "1-in-50yr":  0.70,
    "1-in-100yr": 1.00,
    "1-in-250yr": 1.80,
    "1-in-500yr": 3.00,
}

_CANADIAN_PROVINCES = [
    "Ontario", "Quebec", "British Columbia", "Alberta", "Atlantic",
    "Manitoba", "Saskatchewan", "Nova Scotia", "New Brunswick",
    "Newfoundland", "PEI", "NWT", "Yukon",
]

def save_scenario_annotation(
    segment_id: str,
    note: str,
    analyst: str,
    scenario_type: str = "",
    adjustment_pct: float | None = None,
    approval_status: str = "Draft",
):
    try:
        _ensure_annotations_table()
        conn = get_lakebase_conn()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO public.scenario_annotations "
            "(segment_id, note, analyst, scenario_type, adjustment_pct, approval_status, created_at) "
            "VALUES (%s, %s, %s, %s, %s, %s, NOW())",
            (segment_id, note, analyst,
             scenario_type or None,
             adjustment_pct,
             approval_status or "Draft"),
        )
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        st.warning(f"Could not save annotation: {e}")
        return False

def load_annotations(segment_id: str):
    try:
        _ensure_annotations_table()
        conn = get_lakebase_conn()
        cur = conn.cursor()
        cur.execute(
            "SELECT analyst, scenario_type, adjustment_pct, approval_status, note, created_at "
            "FROM public.scenario_annotations "
            "WHERE segment_id = %s ORDER BY created_at DESC LIMIT 20",
            (segment_id,)
        )
        rows = cur.fetchall()
        conn.close()
        if rows:
            return pd.DataFrame(
                rows,
                columns=["Analyst", "Type", "Adj %", "Status", "Note", "Created At"],
            )
        return pd.DataFrame()
    except Exception:
        return pd.DataFrame()

# â”€â”€â”€ Sidebar: Data & Model Reference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.header("About")

    st.markdown("**Portfolio Data**")
    st.markdown("""
Insurance claims across **40 segments** (product line Ã— Canadian province):

| | |
|---|---|
| **Product lines** | Personal Auto, Commercial Auto, Homeowners, Commercial Property |
| **Provinces** | All 10 Canadian provinces |
| **Period** | Jan 2019 â€“ Dec 2024 (72 months) |
| **Macro data** | StatCan unemployment, housing price index, housing starts |
""")

    st.divider()
    st.markdown("**What the models do**")
    st.markdown("""
| Model | Business purpose |
|---|---|
| **Claims Forecasting** | Projects monthly claim volumes per segment â€” used in the Claims Forecast and Quick Forecast tabs |
| **Volatility Model** | Estimates how unpredictable loss ratios are month-to-month â€” feeds into capital calculations |
| **Monte Carlo Simulation** | Runs millions of loss scenarios to compute capital requirements (Expected Loss, SCR, Tail Risk) â€” powers the Capital Requirements and Stress Testing tabs |
""")

    st.divider()
    st.markdown("**How it's built**")
    st.markdown("""
```
Raw CDC events
  â†’ Bronze (Delta Live Tables)
  â†’ Silver (SCD Type 2 policies)
  â†’ Gold (monthly segment stats)
  â†’ Feature Store (point-in-time joins)
  â†’ SARIMA / GARCH per segment
  â†’ Monte Carlo portfolio simulation
  â†’ UC Model Registry (@Champion)
     â”œâ”€â”€ SARIMA endpoint (Forecasts tab)
     â””â”€â”€ Monte Carlo endpoint (Scenario tab)
  â†’ This App
```
All assets are version-controlled and reproducible.
Model artifacts are logged to MLflow and promoted via
the Unity Catalog Model Registry.
""")

    st.divider()
    st.caption("Powered by Databricks Apps + Streamlit")

# â”€â”€â”€ App Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.title("ğŸ“Š Insurance Portfolio Risk Intelligence")
st.caption("Powered by Databricks | Claims Forecasting Â· Capital Planning Â· Catastrophe Analysis")

tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ“ˆ Claims Forecast", "ğŸ’° Capital Requirements", "âš¡ Quick Forecast", "ğŸ² Stress Testing", "ğŸŒªï¸ Catastrophe & Reserves"])

# â”€â”€ Tab 1: Segment Forecasts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab1:
    st.subheader("Claims Volume Forecast by Product & Region")

    with st.expander("â„¹ï¸ About this forecast", expanded=False):
        st.markdown("""
**What this shows:** Projected monthly claim volumes for the selected product line and region, based on 6 years of historical claims data (Jan 2019 â€“ Dec 2024).

**The shaded band** is the forecast uncertainty range â€” the model expects 95% of actual future months to fall within this range. A wider band means higher uncertainty, which is normal for longer forecast horizons.

**The dashed line** shows the most likely (point estimate) outcome each month.

**Important:** This model assumes the historical seasonal pattern continues. Use the analyst note tool below to flag known upcoming changes â€” new products, regulatory changes, or unusual loss events â€” that the model cannot anticipate automatically.

_Technical details: SARIMA(1,1,1)(1,1,1)â‚â‚‚ fitted per segment using statsmodels.SARIMAX._
""")

    segments = load_segments()
    if segments:
        selected = st.selectbox("Select product line & region:", segments, index=0)

        if selected:
            # Load segment history stats alongside the forecast
            stats_df = load_segment_stats(selected)
            if not stats_df.empty:
                for col in stats_df.columns:
                    stats_df[col] = stats_df[col].apply(
                        lambda x: pd.to_numeric(x, errors='ignore') if x is not None else x
                    )
                s = stats_df.iloc[0]
                sc1, sc2, sc3, sc4 = st.columns(4)
                sc1.metric(
                    "History",
                    f"{str(s['first_month'])[:7]} â€“ {str(s['last_month'])[:7]}",
                    help="Date range of historical claims data used to build this forecast"
                )
                sc2.metric(
                    "Avg Monthly Claims",
                    f"{float(s['avg_monthly_claims']):,.0f}",
                    help="Average number of claims filed per month over the historical period"
                )
                sc3.metric(
                    "Month-to-Month Variability",
                    f"{float(s['stddev_claims']):,.0f}",
                    help="How much monthly claims typically vary. Higher values mean less predictable months."
                )
                sc4.metric(
                    "Range",
                    f"{int(float(s['min_claims'])):,} â€“ {int(float(s['max_claims'])):,}",
                    help="Minimum and maximum observed monthly claims count"
                )

            df = load_forecasts(selected)

            if not df.empty:
                for col in ["claims_count", "forecast_mean", "forecast_lo95", "forecast_hi95"]:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')

                actuals   = df[df["record_type"] == "actual"]
                forecasts = df[df["record_type"] == "forecast"]

                import plotly.graph_objects as go
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=actuals["month"], y=actuals["claims_count"],
                    mode="lines+markers", name="Actual claims",
                    line=dict(color="#1f77b4"),
                    hovertemplate="<b>%{x}</b><br>Actual: %{y:,.0f} claims<extra></extra>",
                ))
                fig.add_trace(go.Scatter(
                    x=forecasts["month"], y=forecasts["forecast_mean"],
                    mode="lines+markers", name="Projected claims",
                    line=dict(color="#FF3419", dash="dash"),
                    hovertemplate="<b>%{x}</b><br>Forecast: %{y:,.0f} claims<extra></extra>",
                ))
                fig.add_trace(go.Scatter(
                    x=pd.concat([forecasts["month"], forecasts["month"][::-1]]),
                    y=pd.concat([forecasts["forecast_hi95"], forecasts["forecast_lo95"][::-1]]),
                    fill="toself", fillcolor="rgba(255,52,25,0.15)",
                    line=dict(color="rgba(255,0,0,0)"),
                    name="Forecast range (95% confidence)",
                    hoverinfo="skip",
                ))
                # Add a vertical marker at the forecast start
                if not actuals.empty and not forecasts.empty:
                    cutoff = str(actuals["month"].max())
                    fig.add_shape(
                        type="line",
                        x0=cutoff, x1=cutoff, y0=0, y1=1, yref="paper",
                        line=dict(dash="dot", color="grey", width=1),
                    )
                    fig.add_annotation(
                        x=cutoff, y=1, yref="paper",
                        text="Forecast start", showarrow=False,
                        yanchor="bottom", font=dict(color="grey", size=11),
                    )
                fig.update_layout(
                    title=f"{selected} â€” Claims Forecast",
                    xaxis_title="Month",
                    yaxis_title="Monthly Claims",
                    height=420,
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                    hovermode="x unified",
                )
                st.plotly_chart(fig, use_container_width=True)

                col1, col2, col3 = st.columns(3)
                if "mape" in df.columns:
                    mape_vals = pd.to_numeric(df[df["record_type"] == "forecast"]["mape"], errors='coerce')
                    col1.metric(
                        "Forecast Accuracy",
                        f"{mape_vals.mean():.1f}%",
                        help="Average error rate on the out-of-sample test period. Lower is better â€” under 10% is strong for insurance data."
                    )
                if not forecasts.empty:
                    col2.metric(
                        "Projected Monthly Claims",
                        f"{int(forecasts['forecast_mean'].mean()):,}",
                        help="Average projected claims per month over the 12-month forecast horizon"
                    )
                    half_width = int((forecasts['forecast_hi95'] - forecasts['forecast_lo95']).mean() / 2)
                    col3.metric(
                        "Forecast Uncertainty Range",
                        f"Â±{half_width:,}",
                        help="Average margin of uncertainty around the monthly forecast. Wider ranges reflect higher uncertainty â€” normal as the forecast extends further into the future."
                    )

                # GARCH volatility overlay
                with st.expander("ğŸ“Š GARCH Volatility Overlay", expanded=False):
                    st.caption(
                        "Conditional volatility from GARCH(1,1) â€” shows how loss ratio variability "
                        "clusters over time. Higher volatility periods indicate greater uncertainty in claims."
                    )
                    garch_df = load_garch_volatility(selected)
                    if not garch_df.empty:
                        import plotly.graph_objects as go
                        fig_garch = go.Figure()
                        fig_garch.add_trace(go.Scatter(
                            x=garch_df["month"], y=garch_df["cond_volatility"],
                            mode="lines", name="Conditional Volatility",
                            line=dict(color="#FF6B35", width=2),
                            hovertemplate="<b>%{x}</b><br>Volatility: %{y:.3f}<extra></extra>",
                        ))
                        fig_garch.update_layout(
                            title=f"{selected} â€” GARCH(1,1) Conditional Volatility",
                            xaxis_title="Month",
                            yaxis_title="Conditional Volatility (loss ratio log-returns)",
                            height=300,
                            hovermode="x unified",
                        )
                        st.plotly_chart(fig_garch, use_container_width=True)
                    else:
                        st.info("GARCH volatility data not yet available. Run Module 4 to generate.")

                with st.expander("ğŸ“‹ Raw forecast data"):
                    st.markdown("""
| Column | Description |
|---|---|
| `month` | Calendar month (YYYY-MM-DD, first of month) |
| `record_type` | `actual` = observed history; `forecast` = SARIMA prediction |
| `claims_count` | Observed monthly claims count (actuals only) |
| `forecast_mean` | Point forecast â€” mean of the predictive distribution |
| `forecast_lo95` | Lower bound of 95% prediction interval |
| `forecast_hi95` | Upper bound of 95% prediction interval |
""")
                    display_df = df.copy()
                    display_df.columns = [c.replace("_", " ").title() for c in display_df.columns]
                    st.dataframe(display_df, use_container_width=True, hide_index=True)

        # Scenario annotation
        with st.expander("ğŸ“ Add Analyst Note"):
            st.caption(
                "Record any known factors that could affect this forecast â€” new products, regulatory changes, "
                "loss events, or expert judgment overrides. Notes are saved and visible to all team members."
            )
            # Pre-populate analyst name from the forwarded user token if available
            _user_token = st.context.headers.get("X-Forwarded-Access-Token", "")
            _default_analyst = _email_from_token(_user_token) if _user_token else ""

            _an_col1, _an_col2 = st.columns(2)
            with _an_col1:
                analyst       = st.text_input("Analyst:", value=_default_analyst)
                scenario_type = st.selectbox("Type:", _SCENARIO_TYPES)
            with _an_col2:
                approval_status = st.selectbox("Status:", _APPROVAL_STATUSES)
                adjustment_pct  = st.number_input(
                    "Recommended Forecast Adjustment (%):",
                    min_value=-50.0, max_value=50.0, value=0.0, step=0.5,
                    help="Enter a positive % to revise the forecast upward, or negative to revise downward. Use 0 if no adjustment is needed.",
                )
            note = st.text_area("Notes:")
            if st.button("Save Note"):
                adj = adjustment_pct if adjustment_pct != 0.0 else None
                if save_scenario_annotation(selected, note, analyst,
                                            scenario_type, adj, approval_status):
                    st.success("Note saved")

        with st.expander("ğŸ“‹ View Previous Notes"):
            if selected:
                annotations = load_annotations(selected)
                if not annotations.empty:
                    st.dataframe(annotations, use_container_width=True, hide_index=True)
                else:
                    st.info("No annotations yet for this segment.")
    else:
        st.warning("No data found. The pipeline may still be initialising â€” please check back in a few minutes.")

# â”€â”€ Tab 2: Portfolio Risk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab2:
    st.subheader("Annual Capital Requirements")
    st.caption("Based on 40 million simulated loss scenarios across Commercial Property, Commercial Auto, and Liability")

    with st.expander("â„¹ï¸ About these numbers", expanded=False):
        st.markdown("""
**What this shows:** How much capital your portfolio needs to hold at different risk tolerance levels, based on 40 million simulated loss scenarios.

**The four risk levels:**
- **Expected Annual Loss** â€” What you should budget for in a typical year (long-run average)
- **1-in-100 Year Loss** â€” The loss level you'd only expect to exceed once a century. A common internal risk appetite threshold.
- **Solvency Capital Requirement (1-in-200 Year)** â€” The Solvency II regulatory capital benchmark. Insurers must hold capital sufficient to survive this level with 99.5% confidence.
- **Tail Risk Estimate** â€” The average loss across the very worst scenarios (beyond the 1-in-100 threshold). More conservative than VaR because it captures how bad things get in extreme years.

**Why losses cluster and spike:** Most years sit near the expected loss, but a small number of scenarios produce losses many times larger. This "heavy right tail" is normal in insurance â€” it's why catastrophe reinsurance and capital buffers exist.

**How lines of business interact:** Losses are modelled as correlated â€” a widespread event (e.g., a major storm) can hit Property and Auto simultaneously, compounding total losses beyond what either line would produce alone.

_Technical: t-Copula (df=4) + lognormal marginals. Correlation matrix: Ï(Prop,Auto)=0.40, Ï(Prop,Liab)=0.20, Ï(Auto,Liab)=0.30._
""")

    summary = load_monte_carlo_summary()

    col1, col2, col3, col4 = st.columns(4)
    col1.metric(
        "Expected Annual Loss",
        f"${summary['expected_loss']:.1f}M",
        help="What the portfolio is expected to cost in a typical year â€” the long-run average across all simulated scenarios. Use this as your base budget assumption."
    )
    col2.metric(
        "1-in-100 Year Loss",
        f"${summary['var_99']:.1f}M",
        help="The loss level exceeded in only 1% of scenarios. Think of it as the worst year in a century. A common risk appetite benchmark for internal capital planning."
    )
    col3.metric(
        "Solvency Capital Requirement",
        f"${summary['var_995']:.1f}M",
        help="The Solvency II regulatory capital threshold (SCR) â€” the 1-in-200 year loss level. Insurers must hold enough capital to cover losses at this level with 99.5% confidence."
    )
    col4.metric(
        "Tail Risk Estimate",
        f"${summary['cvar_99']:.1f}M",
        help="The average loss across the worst 1% of scenarios â€” what you'd expect to lose when things go really wrong, beyond the 1-in-100 threshold. More conservative than VaR and used in IFRS 17 risk margin calculations."
    )

    st.divider()

    # Risk metric comparison chart
    metrics = {
        "Expected\nAnnual Loss": float(summary["expected_loss"]),
        "1-in-100\nYear Loss": float(summary["var_99"]),
        "Solvency Capital\nRequirement": float(summary["var_995"]),
        "Tail Risk\nEstimate": float(summary["cvar_99"]),
    }
    import plotly.graph_objects as go
    colors = ["#1f77b4", "#ff7f0e", "#d62728", "#9467bd"]
    fig2 = go.Figure(go.Bar(
        x=list(metrics.keys()),
        y=list(metrics.values()),
        marker_color=colors,
        text=[f"${v:.1f}M" for v in metrics.values()],
        textposition="outside",
        hovertemplate="%{x}<br><b>$%{y:.1f}M</b><extra></extra>",
    ))
    fig2.update_layout(
        title="Capital Required at Each Risk Level",
        yaxis_title="Annual Loss ($M)",
        height=380,
        showlegend=False,
        yaxis=dict(range=[0, max(metrics.values()) * 1.25]),
    )
    st.plotly_chart(fig2, use_container_width=True)

    # Simulated distribution using the per-row mean losses as a proxy histogram
    dist_df = load_monte_carlo_distribution()
    if not dist_df.empty:
        for col in dist_df.columns:
            dist_df[col] = pd.to_numeric(dist_df[col], errors='coerce')

        with st.expander("ğŸ“Š How Losses Are Distributed Across Scenarios", expanded=True):
            st.markdown("""
Each bar shows how many of the 40 million simulated years produced losses in that range. The vertical lines mark key capital thresholds.

Most simulated years cluster near the expected loss â€” but a long tail of rare, high-loss years stretches to the right. This is the fundamental reason insurance companies hold capital buffers well above their average annual loss.
""")
            fig3 = go.Figure()
            fig3.add_trace(go.Histogram(
                x=dist_df["mean_loss_M"],
                nbinsx=60,
                name="Simulated scenarios",
                marker_color="rgba(31,119,180,0.6)",
                hovertemplate="Loss: $%{x:.1f}M<br>Count: %{y}<extra></extra>",
            ))
            for label, val, color in [
                ("Expected Loss", float(summary["expected_loss"]), "#2ca02c"),
                ("1-in-100yr", float(summary["var_99"]), "#ff7f0e"),
                ("SCR (1-in-200yr)", float(summary["var_995"]), "#d62728"),
            ]:
                fig3.add_vline(
                    x=val, line_dash="dash", line_color=color,
                    annotation_text=f"{label}: ${val:.1f}M",
                    annotation_position="top right",
                )
            fig3.update_layout(
                xaxis_title="Annual Portfolio Loss ($M)",
                yaxis_title="Number of Scenarios",
                height=360,
                showlegend=False,
            )
            st.plotly_chart(fig3, use_container_width=True)

    st.markdown("""
> **Regulatory context:** The Solvency Capital Requirement (SCR) is the amount of capital a Solvency II insurer must hold to survive a 1-in-200 year loss event. The Tail Risk Estimate goes further â€” capturing the average severity of scenarios beyond that threshold, and is increasingly referenced in IFRS 17 risk margin calculations.
""")

# â”€â”€ Tab 3: On-Demand Forecast â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab3:
    st.subheader("Generate a Custom Claims Forecast")
    st.caption("Get an instant forecast for any time horizon â€” results update in seconds")

    with st.expander("â„¹ï¸ How this works", expanded=False):
        st.markdown("""
**What this does:** Generates a claims forecast on demand for any horizon up to 24 months, without re-running the full modelling pipeline.

**When to use it:**
- Quick "what if I need a 18-month forecast?" checks
- Board or management presentations requiring a specific horizon
- Validating that the deployed model is live and responding correctly

**What you get back:**
- A monthly point forecast (most likely outcome)
- Upper and lower bounds of the 95% forecast range
- A note if uncertainty grows significantly at longer horizons (expected â€” forecasts become less precise the further out you go)

**The model** is the same one trained on all 40 segments â€” it represents aggregate portfolio behaviour. For segment-specific forecasts with full historical context, use the **Claims Forecast** tab.

_Technical: SARIMA REST endpoint served via Databricks Model Serving, @Champion alias in Unity Catalog._
""")

    horizon = st.slider(
        "How many months ahead to forecast:",
        min_value=1, max_value=24, value=6,
        help="How many months ahead to forecast. Uncertainty (CI width) grows with horizon."
    )

    if st.button("Generate Forecast"):
        with st.spinner("Calling Model Serving endpoint..."):
            _fetched = call_serving_endpoint(horizon)
            if not _fetched.empty:
                st.session_state["ondemand_result"] = _fetched
                st.session_state["ondemand_horizon"] = horizon
            else:
                st.warning("Endpoint not available â€” start the Model Serving endpoint from Module 5")

    result_df = st.session_state.get("ondemand_result", pd.DataFrame())
    _display_horizon = st.session_state.get("ondemand_horizon", horizon)
    if not result_df.empty:
        # Rename for display
        display_cols = {
            "month_offset": "Month Ahead",
            "forecast_mean": "Point Forecast (mean)",
            "forecast_lo95": "Lower 95% CI",
            "forecast_hi95": "Upper 95% CI",
        }
        display_df = result_df.rename(columns={k: v for k, v in display_cols.items() if k in result_df.columns})

        # Numeric formatting
        for col in display_df.columns:
            if col != "Month Ahead":
                display_df[col] = pd.to_numeric(display_df[col], errors='coerce').round(1)

        st.dataframe(display_df, use_container_width=True, hide_index=True)

        # Visualise the forecast
        if all(c in result_df.columns for c in ["month_offset", "forecast_mean", "forecast_lo95", "forecast_hi95"]):
            result_df["forecast_mean"] = pd.to_numeric(result_df["forecast_mean"], errors='coerce')
            result_df["forecast_lo95"] = pd.to_numeric(result_df["forecast_lo95"], errors='coerce')
            result_df["forecast_hi95"] = pd.to_numeric(result_df["forecast_hi95"], errors='coerce')

            fig4 = go.Figure()
            fig4.add_trace(go.Scatter(
                x=result_df["month_offset"], y=result_df["forecast_mean"],
                mode="lines+markers", name="Projected claims",
                line=dict(color="#FF3419"),
                hovertemplate="Month +%{x}<br>Projected: %{y:,.1f} claims<extra></extra>",
            ))
            fig4.add_trace(go.Scatter(
                x=pd.concat([result_df["month_offset"], result_df["month_offset"][::-1]]),
                y=pd.concat([result_df["forecast_hi95"], result_df["forecast_lo95"][::-1]]),
                fill="toself", fillcolor="rgba(255,52,25,0.15)",
                line=dict(color="rgba(255,0,0,0)"),
                name="Forecast range",
                hoverinfo="skip",
            ))
            fig4.update_layout(
                title=f"Claims Forecast â€” Next {_display_horizon} Months",
                xaxis_title="Month",
                yaxis_title="Projected Monthly Claims",
                height=360,
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig4, use_container_width=True)

            # Flag widening CI
            ci_width_start = float(result_df["forecast_hi95"].iloc[0] - result_df["forecast_lo95"].iloc[0])
            ci_width_end   = float(result_df["forecast_hi95"].iloc[-1] - result_df["forecast_lo95"].iloc[-1])
            if ci_width_end > ci_width_start * 1.5:
                st.info(
                    "The forecast range widens significantly at longer horizons â€” this is expected. "
                    "Uncertainty compounds over time; use shorter-horizon forecasts for operational decisions "
                    "and longer horizons for strategic planning only."
                )

        st.download_button(
            "Download CSV",
            result_df.to_csv(index=False),
            file_name=f"sarima_forecast_{_display_horizon}m.csv",
            mime="text/csv",
        )

# â”€â”€ Tab 4: Scenario Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab4:
    st.subheader("Custom Stress Test")
    st.caption("Model the capital impact of any loss scenario in seconds")

    with st.expander("â„¹ï¸ How this works", expanded=False):
        st.markdown("""
**What this does:** Lets you change the loss assumptions for any line of business and immediately see how your capital requirements change. Use it to answer questions like:

- "What happens to our SCR if property losses increase 20%?"
- "How much extra capital do we need if losses become more volatile?"
- "What's the capital impact of a widespread event that hits Property and Auto at the same time?"

**The three groups of inputs:**
- **Expected Annual Losses** â€” Your best estimate of what each line will cost in the scenario. Raise these to model a hard market, higher exposure, or adverse claims trends.
- **Loss Volatility** â€” How unpredictable each line is. Higher volatility means wider loss distributions and higher capital requirements.
- **How Lines Move Together** â€” Whether multiple lines tend to have bad years at the same time. Higher values mean more losses pile up during stress events, increasing required capital.

**Results** show how your capital requirements (Expected Loss, 1-in-100, SCR, Tail Risk) change versus the pre-computed baseline.

_Technical: t-Copula Monte Carlo endpoint, same model as the Portfolio Risk tab._
""")

    # â”€â”€ Parameter inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("#### Loss Assumptions")

    col_means, col_cv, col_corr = st.columns(3)

    with col_means:
        st.markdown("**Expected Annual Losses ($M)**")
        mean_prop = st.number_input("Commercial Property", value=12.5, min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $12.5M. Increase to model higher property losses â€” hard market, increased building values, or adverse claims trends.")
        mean_auto = st.number_input("Commercial Auto",     value=8.3,  min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $8.3M. Increase to model more frequent or severe auto claims.")
        mean_liab = st.number_input("Liability",           value=5.7,  min_value=0.1, max_value=500.0, step=0.5,
                                    help="Baseline: $5.7M. Increase to model higher liability exposure or adverse reserve development.")

    with col_cv:
        st.markdown("**Loss Volatility (unpredictability)**")
        cv_prop = st.slider("Property Volatility", min_value=0.05, max_value=2.0, value=0.35, step=0.05,
                            help="Baseline: 0.35. Higher values mean more unpredictable losses and higher capital requirements.")
        cv_auto = st.slider("Auto Volatility",     min_value=0.05, max_value=2.0, value=0.28, step=0.05,
                            help="Baseline: 0.28. Auto claims tend to be the most stable line.")
        cv_liab = st.slider("Liability Volatility", min_value=0.05, max_value=2.0, value=0.42, step=0.05,
                            help="Baseline: 0.42. Liability is naturally more volatile due to long development tails and legal uncertainty.")

    with col_corr:
        st.markdown("**How Lines Move Together**")
        corr_pa = st.slider("Property â†” Auto",      min_value=0.0, max_value=0.95, value=0.40, step=0.05,
                            help="Baseline: 0.40. How often Property and Auto have bad years at the same time. Raise to 0.6â€“0.8 to model a widespread event like a storm affecting both.")
        corr_pl = st.slider("Property â†” Liability", min_value=0.0, max_value=0.95, value=0.20, step=0.05,
                            help="Baseline: 0.20. Property and Liability losses are relatively independent in normal conditions.")
        corr_al = st.slider("Auto â†” Liability",     min_value=0.0, max_value=0.95, value=0.30, step=0.05,
                            help="Baseline: 0.30. Auto and Liability sometimes move together â€” e.g., an economic downturn affecting both claims frequency and litigation.")

    col_sim1, col_sim2 = st.columns(2)
    with col_sim1:
        n_scen = st.select_slider(
            "Simulation Precision",
            options=[1_000, 5_000, 10_000, 25_000, 50_000],
            value=10_000,
            help="More paths = more accurate results, takes slightly longer. 10,000 is good for most analyses.",
        )
    with col_sim2:
        copula_df_val = st.select_slider(
            "Tail Risk Sensitivity",
            options=[3, 4, 5, 10, 20, 30],
            value=4,
            help="Controls how severely multiple lines are affected simultaneously during extreme events. Lower = more conservative (heavier simultaneous losses). Default of 4 is the actuarial calibration; 30 approximates independent lines.",
        )

    # â”€â”€ Baseline reference (read from Delta table, cached) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _baseline_summary = load_monte_carlo_summary()

    # â”€â”€ Run button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.button("Run Scenario", type="primary"):
        _scenario = {
            "mean_property_M":  mean_prop,
            "mean_auto_M":      mean_auto,
            "mean_liability_M": mean_liab,
            "cv_property":      cv_prop,
            "cv_auto":          cv_auto,
            "cv_liability":     cv_liab,
            "corr_prop_auto":   corr_pa,
            "corr_prop_liab":   corr_pl,
            "corr_auto_liab":   corr_al,
            "n_scenarios":      n_scen,
            "copula_df":        copula_df_val,
        }
        with st.spinner(f"Running Monte Carlo ({n_scen:,} scenarios)..."):
            _fetched = call_monte_carlo_endpoint(_scenario)
        if _fetched:
            st.session_state["scenario_result"] = _fetched
        else:
            st.warning(
                "Monte Carlo endpoint not available. "
                "Start it from Module 6 or wait for the setup job to complete."
            )

    _result = st.session_state.get("scenario_result")
    if _result:
        st.success("Scenario complete")
        st.divider()

        # â”€â”€ Risk metrics comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.markdown("#### Capital Impact: Scenario vs. Baseline")

        _b = _baseline_summary  # Series from load_monte_carlo_summary()

        def _delta(scenario_val, baseline_val):
            if baseline_val and baseline_val != 0:
                d = scenario_val - float(baseline_val)
                return f"{'+' if d >= 0 else ''}${d:.1f}M"
            return None

        rc1, rc2, rc3, rc4 = st.columns(4)
        rc1.metric(
            "Expected Annual Loss",
            f"${_result['expected_loss_M']:.1f}M",
            delta=_delta(_result['expected_loss_M'], _b.get('expected_loss', 0)),
            help="What the portfolio is expected to cost per year under these assumptions.",
        )
        rc2.metric(
            "1-in-100 Year Loss",
            f"${_result['var_99_M']:.1f}M",
            delta=_delta(_result['var_99_M'], _b.get('var_99', 0)),
            help="Loss level exceeded only once a century under this scenario.",
        )
        rc3.metric(
            "Solvency Capital Requirement",
            f"${_result['var_995_M']:.1f}M",
            delta=_delta(_result['var_995_M'], _b.get('var_995', 0)),
            help="Required capital under Solvency II for this scenario.",
        )
        rc4.metric(
            "Tail Risk Estimate",
            f"${_result['cvar_99_M']:.1f}M",
            delta=_delta(_result['cvar_99_M'], _b.get('cvar_99', 0)),
            help="Average loss in the very worst outcomes â€” beyond the 1-in-100 threshold.",
        )

        # â”€â”€ Waterfall chart: Scenario vs Baseline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        import plotly.graph_objects as go

        _metrics_labels = ["Expected\nAnnual Loss", "VaR 95%", "1-in-100\nYear Loss", "Solvency Capital\nRequirement", "Tail Risk\nEstimate"]
        _baseline_vals = [
            float(_b.get("expected_loss", 0)),
            float(_b.get("var_99", 0)) * 0.85,   # approximate VaR95 from VaR99
            float(_b.get("var_99", 0)),
            float(_b.get("var_995", 0)),
            float(_b.get("cvar_99", 0)),
        ]
        _scenario_vals = [
            _result["expected_loss_M"],
            _result["var_95_M"],
            _result["var_99_M"],
            _result["var_995_M"],
            _result["cvar_99_M"],
        ]

        _fig_cmp = go.Figure()
        _fig_cmp.add_trace(go.Bar(
            name="Baseline",
            x=_metrics_labels,
            y=_baseline_vals,
            marker_color="rgba(31,119,180,0.7)",
            hovertemplate="%{x}<br>Baseline: $%{y:.1f}M<extra></extra>",
        ))
        _fig_cmp.add_trace(go.Bar(
            name="Stress Scenario",
            x=_metrics_labels,
            y=_scenario_vals,
            marker_color="rgba(214,39,40,0.7)",
            hovertemplate="%{x}<br>Scenario: $%{y:.1f}M<extra></extra>",
        ))
        _fig_cmp.update_layout(
            title="Capital Requirement Change vs. Baseline",
            yaxis_title="Annual Portfolio Loss ($M)",
            barmode="group",
            height=380,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        )
        st.plotly_chart(_fig_cmp, use_container_width=True)

        # â”€â”€ Raw metrics table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.expander("ğŸ“‹ Detailed results"):
            _display = {
                "Metric": ["Expected Loss", "VaR (95%)", "1-in-100 Year Loss", "Solvency Capital Req.", "Tail Risk Estimate", "Max Loss"],
                "Scenario ($M)": [
                    f"${_result['expected_loss_M']:.2f}",
                    f"${_result['var_95_M']:.2f}",
                    f"${_result['var_99_M']:.2f}",
                    f"${_result['var_995_M']:.2f}",
                    f"${_result['cvar_99_M']:.2f}",
                    f"${_result['max_loss_M']:.2f}",
                ],
                "Copula": [_result.get("copula", ""), "", "", "", "", ""],
                "Scenarios": [f"{int(_result.get('n_scenarios_used', n_scen)):,}", "", "", "", "", ""],
            }
            st.dataframe(pd.DataFrame(_display), use_container_width=True, hide_index=True)

# â”€â”€ Tab 5: Catastrophe Scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with tab5:
    st.subheader("Catastrophe Event Analysis")
    st.caption(
        "Pre-modelled natural disasters and market events, plus custom catastrophe simulation"
    )

    import plotly.graph_objects as go

    # â”€â”€ Section 1: Pre-computed stress scenarios â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### Pre-Modelled Catastrophe Stress Tests")
    st.caption(
        "Pre-run at deployment across 3 major stress scenarios â€” 120 million simulated loss paths each."
    )

    _baseline_smry = load_monte_carlo_summary()
    _stress_df     = load_stress_scenarios()

    if not _stress_df.empty:
        _b_var995  = float(_baseline_smry.get('var_995', 0))
        _b_var99   = float(_baseline_smry.get('var_99', 0))
        _b_cvar99  = float(_baseline_smry.get('cvar_99', 0))
        _b_mean    = float(_baseline_smry.get('expected_loss', 0))

        _disp_rows = [{"Scenario": "Baseline", "Exp. Loss": f"${_b_mean:.1f}M",
                       "VaR(99%)": f"${_b_var99:.1f}M", "VaR(99.5%) SCR": f"${_b_var995:.1f}M",
                       "CVaR(99%)": f"${_b_cvar99:.1f}M", "Î” SCR": "â€”"}]
        for _, row in _stress_df.iterrows():
            _disp_rows.append({
                "Scenario":       row["scenario_label"],
                "Exp. Loss":      f"${row['total_mean_M']:.1f}M",
                "VaR(99%)":       f"${row['var_99_M']:.1f}M",
                "VaR(99.5%) SCR": f"${row['var_995_M']:.1f}M",
                "CVaR(99%)":      f"${row['cvar_99_M']:.1f}M",
                "Î” SCR":   f"{row['var_995_vs_baseline']:+.1f}%",
            })
        st.dataframe(pd.DataFrame(_disp_rows), use_container_width=True, hide_index=True)

        _all_labels = ["Baseline"] + _stress_df["scenario_label"].tolist()
        _all_var995 = [_b_var995] + _stress_df["var_995_M"].tolist()
        _all_cvar99 = [_b_cvar99] + _stress_df["cvar_99_M"].tolist()
        _bar_colors = ["#1f77b4", "#ff7f0e", "#d62728", "#9467bd"]
        _bar_colors_muted = [
            f"rgba({int(c[1:3],16)},{int(c[3:5],16)},{int(c[5:7],16)},0.5)"
            for c in _bar_colors
        ]

        _fig_stress = go.Figure()
        _fig_stress.add_trace(go.Bar(
            name="VaR(99.5%) â€” SCR",
            x=_all_labels, y=_all_var995,
            marker_color=_bar_colors,
            text=[f"${v:.1f}M" for v in _all_var995], textposition="outside",
            hovertemplate="%{x}<br>VaR(99.5%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_stress.add_trace(go.Bar(
            name="CVaR(99%)",
            x=_all_labels, y=_all_cvar99,
            marker_color=_bar_colors_muted,
            text=[f"${v:.1f}M" for v in _all_cvar99], textposition="outside",
            hovertemplate="%{x}<br>CVaR(99%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_stress.update_layout(
            title="VaR(99.5%) and CVaR(99%) â€” Baseline vs. Stress Scenarios",
            yaxis_title="Annual Portfolio Loss ($M)",
            barmode="group", height=400, showlegend=True,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            yaxis=dict(range=[0, max(_all_var995 + _all_cvar99) * 1.25]),
        )
        st.plotly_chart(_fig_stress, use_container_width=True)
    else:
        st.info("Stress scenario data not yet available â€” run the setup job (e2-demo-ray target).")

    # â”€â”€ Section 2: VaR evolution timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### Capital Requirement Outlook â€” Next 12 Months")
    _timeline_df = load_var_timeline()
    if not _timeline_df.empty:
        _fig_tl = go.Figure()
        _fig_tl.add_trace(go.Scatter(
            x=_timeline_df["month_idx"], y=_timeline_df["var_995_M"],
            mode="lines+markers", name="VaR(99.5%)", line=dict(color="#d62728"),
            hovertemplate="Month +%{x}<br>VaR(99.5%): $%{y:.1f}M<extra></extra>",
        ))
        _fig_tl.add_trace(go.Scatter(
            x=_timeline_df["month_idx"], y=_timeline_df["var_99_M"],
            mode="lines+markers", name="VaR(99%)", line=dict(color="#ff7f0e", dash="dash"),
            hovertemplate="Month +%{x}<br>VaR(99%): $%{y:.1f}M<extra></extra>",
        ))
        _b_v995 = float(_baseline_smry.get('var_995', 0))
        if _b_v995 > 0:
            _fig_tl.add_hline(
                y=_b_v995, line_dash="dot", line_color="grey",
                annotation_text=f"Current VaR(99.5%): ${_b_v995:.1f}M",
                annotation_position="bottom right",
            )
        _fig_tl.update_layout(
            title="How Capital Requirements Are Projected to Change Over the Next 12 Months",
            xaxis_title="Month", yaxis_title="Required Capital ($M)",
            height=380,
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        )
        st.plotly_chart(_fig_tl, use_container_width=True)
    else:
        st.info("Capital outlook not yet available. Run the full setup job to generate this view.")

    st.divider()

    # â”€â”€ Section 3: Custom CAT scenario submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("### Model a Custom Catastrophe Event")
    st.caption(
        "Configure a natural disaster or market event and immediately see its capital impact. Results are saved to the audit log."
    )

    _cat_top1, _cat_top2 = st.columns(2)
    with _cat_top1:
        _cat_type = st.selectbox(
            "Event type:",
            list(_CAT_PRESETS.keys()),
            help="Each event type pre-fills realistic loss assumptions for that type of disaster. You can adjust them in the parameters section below.",
        )
        st.caption(f"_{_CAT_PRESETS[_cat_type]['desc']}_")
        _return_period = st.selectbox(
            "Event Severity:",
            list(_RETURN_PERIOD_MULT.keys()),
            index=1,
            help="How rare and severe the event is. 1-in-250yr is the Solvency II catastrophe benchmark; 1-in-500yr represents an extreme stress test.",
        )
    with _cat_top2:
        _affected_regions = st.multiselect(
            "Affected Regions:",
            _CANADIAN_PROVINCES,
            default=["Ontario", "Quebec"],
            help="Which provinces are exposed to this event â€” used for documentation and the audit log.",
        )
        _affected_lines = st.multiselect(
            "Affected Lines of Business:",
            ["Commercial Property", "Commercial Auto", "Personal Auto", "Homeowners", "Liability"],
            default=["Commercial Property", "Commercial Auto"],
            help="Which lines of business are most directly hit â€” used for documentation.",
        )

    # Compute default params from preset + return period
    _preset   = _CAT_PRESETS[_cat_type]
    _rp_mult  = _RETURN_PERIOD_MULT[_return_period]
    _bm, _bc, _bco = [12.5, 8.3, 5.7], [0.35, 0.28, 0.42], [0.40, 0.20, 0.30]
    _def_means = [min(round(m * mu * _rp_mult, 1), 2000.0) for m, mu in zip(_bm, _preset["means_mult"])]
    _def_cvs   = [min(round(c * cu, 2), 3.0) for c, cu in zip(_bc, _preset["cv_mult"])]
    _def_corrs = [min(c + _preset["corr_add"], 0.95) for c in _bco]

    with st.expander("âš™ï¸ Fine-tune loss assumptions", expanded=False):
        _adj1, _adj2, _adj3 = st.columns(3)
        with _adj1:
            st.markdown("**Expected Annual Losses ($M)**")
            _cat_mean_prop = st.number_input("Property", value=_def_means[0], min_value=0.1, max_value=2000.0, step=1.0, key="cat_mp")
            _cat_mean_auto = st.number_input("Auto",     value=_def_means[1], min_value=0.1, max_value=2000.0, step=1.0, key="cat_ma")
            _cat_mean_liab = st.number_input("Liability",value=_def_means[2], min_value=0.1, max_value=2000.0, step=1.0, key="cat_ml")
        with _adj2:
            st.markdown("**Coefficients of Variation**")
            _cat_cv_prop = st.slider("Property CV", 0.05, 3.0, _def_cvs[0], 0.05, key="cat_cvp")
            _cat_cv_auto = st.slider("Auto CV",     0.05, 3.0, _def_cvs[1], 0.05, key="cat_cva")
            _cat_cv_liab = st.slider("Liability CV",0.05, 3.0, _def_cvs[2], 0.05, key="cat_cvl")
        with _adj3:
            st.markdown("**Inter-Line Correlations**")
            _cat_corr_pa = st.slider("Property â†” Auto",      0.0, 0.95, _def_corrs[0], 0.05, key="cat_cpa")
            _cat_corr_pl = st.slider("Property â†” Liability", 0.0, 0.95, _def_corrs[1], 0.05, key="cat_cpl")
            _cat_corr_al = st.slider("Auto â†” Liability",     0.0, 0.95, _def_corrs[2], 0.05, key="cat_cal")
    _cat_n_scen = st.select_slider(
        "Simulation paths:",
        options=[5_000, 10_000, 25_000, 50_000],
        value=25_000,
        help="25,000 recommended for catastrophe scenarios â€” good balance of accuracy and speed.",
        key="cat_nscen",
    )

    _user_tok5 = st.context.headers.get("X-Forwarded-Access-Token", "")
    _cat_analyst = _email_from_token(_user_tok5) if _user_tok5 else ""
    _cat_analyst_in = st.text_input("Analyst:", value=_cat_analyst, key="cat_analyst")
    _cat_note_in    = st.text_area(
        "Scenario rationale / assumptions:",
        placeholder=f"Describe the {_cat_type} scenario â€” affected exposure, basis for severity, any expert judgment applied...",
        key="cat_note",
    )

    if st.button("ğŸŒªï¸ Run Catastrophe Scenario", type="primary"):
        _cat_scenario_params = {
            "mean_property_M":  _cat_mean_prop,
            "mean_auto_M":      _cat_mean_auto,
            "mean_liability_M": _cat_mean_liab,
            "cv_property":      _cat_cv_prop,
            "cv_auto":          _cat_cv_auto,
            "cv_liability":     _cat_cv_liab,
            "corr_prop_auto":   _cat_corr_pa,
            "corr_prop_liab":   _cat_corr_pl,
            "corr_auto_liab":   _cat_corr_al,
            "n_scenarios":      _cat_n_scen,
            "copula_df":        4,
        }
        with st.spinner(f"Running {_cat_type} ({_return_period}) â€” {_cat_n_scen:,} scenarios..."):
            _cat_result = call_monte_carlo_endpoint(_cat_scenario_params)

        if _cat_result:
            st.success(f"âœ… {_cat_type} scenario complete")

            _b5 = _baseline_smry

            def _cat_delta(sc_val, base_key):
                bv = float(_b5.get(base_key, 0))
                d  = sc_val - bv
                return f"{'+' if d >= 0 else ''}${d:.1f}M"

            _cr1, _cr2, _cr3, _cr4 = st.columns(4)
            _cr1.metric("Expected Annual Loss",     f"${_cat_result['expected_loss_M']:.1f}M",
                        delta=_cat_delta(_cat_result['expected_loss_M'], 'expected_loss'))
            _cr2.metric("1-in-100 Year Loss",          f"${_cat_result['var_99_M']:.1f}M",
                        delta=_cat_delta(_cat_result['var_99_M'], 'var_99'))
            _cr3.metric("Solvency Capital Requirement",  f"${_cat_result['var_995_M']:.1f}M",
                        delta=_cat_delta(_cat_result['var_995_M'], 'var_995'))
            _cr4.metric("Tail Risk Estimate",         f"${_cat_result['cvar_99_M']:.1f}M",
                        delta=_cat_delta(_cat_result['cvar_99_M'], 'cvar_99'))

            # Save annotation to Lakebase
            _var_lift = ((_cat_result['var_995_M'] / max(float(_b5.get('var_995', 1)), 0.01)) - 1.0) * 100
            _full_note = (
                f"[{_cat_type} | {_return_period}] "
                f"Regions: {', '.join(_affected_regions) if _affected_regions else 'N/A'} | "
                f"Lines: {', '.join(_affected_lines) if _affected_lines else 'N/A'} | "
                f"Prop ${_cat_mean_prop:.1f}M/CV={_cat_cv_prop:.2f}, "
                f"Auto ${_cat_mean_auto:.1f}M/CV={_cat_cv_auto:.2f}, "
                f"Liab ${_cat_mean_liab:.1f}M/CV={_cat_cv_liab:.2f} | "
                f"VaR(99.5%)=${_cat_result['var_995_M']:.1f}M, CVaR=${_cat_result['cvar_99_M']:.1f}M"
                + (f" | {_cat_note_in}" if _cat_note_in else "")
            )
            if save_scenario_annotation(
                segment_id="CAT_SCENARIO",
                note=_full_note,
                analyst=_cat_analyst_in,
                scenario_type=f"Catastrophe: {_cat_type}",
                adjustment_pct=round(_var_lift, 1),
                approval_status="Draft",
            ):
                st.caption("âœ“ Scenario saved to audit log")
        else:
            st.warning(
                "Monte Carlo endpoint not available. "
                "Start it from Module 6 or wait for the setup job to complete."
            )

    # Recent CAT scenarios
    st.divider()
    st.markdown("### Recent Catastrophe Scenario Audit Log")
    _cat_history = load_annotations("CAT_SCENARIO")
    if not _cat_history.empty:
        st.dataframe(_cat_history, use_container_width=True, hide_index=True)
    else:
        st.info("No catastrophe scenarios submitted yet. Use the form above to run one.")

    # â”€â”€ Section 4: Regional Forecast Context â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.markdown("### Regional Claims Forecast")
    st.caption(
        "Geographic breakdown of projected claims over the next 12 months â€” shows where "
        "exposure is concentrated and helps contextualize catastrophe stress scenarios."
    )
    _reg_forecast = load_regional_forecast()
    if not _reg_forecast.empty:
        import plotly.express as px
        # Pivot to get regions as rows, months as value
        _reg_total = _reg_forecast.groupby("region")["total_forecast_claims"].sum().reset_index()
        _reg_total = _reg_total.sort_values("total_forecast_claims", ascending=True)
        _fig_reg = px.bar(
            _reg_total, x="total_forecast_claims", y="region",
            orientation="h",
            title="12-Month Cumulative Projected Claims by Region",
            labels={"total_forecast_claims": "Total Projected Claims", "region": "Region"},
        )
        _fig_reg.update_layout(height=400)
        st.plotly_chart(_fig_reg, use_container_width=True)

        with st.expander("ğŸ“‹ Regional forecast detail"):
            st.dataframe(_reg_forecast, use_container_width=True, hide_index=True)
    else:
        st.info("Regional forecast data not yet available. Run Module 4 (Ray target) to generate.")

    # â”€â”€ Section 5: Reserve Development Triangle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.divider()
    st.markdown("### Loss Development Triangle")
    st.caption(
        "Standard actuarial exhibit showing how claims reserves develop over time. "
        "Each row is an accident month; columns show cumulative paid at each development lag."
    )
    _triangle_data = load_reserve_triangle()
    if not _triangle_data.empty:
        # Let user select a product line to view
        _tri_products = sorted(_triangle_data["product_line"].unique())
        _tri_selected = st.selectbox("Product line:", _tri_products, key="tri_prod")
        _tri_filtered = _triangle_data[_triangle_data["product_line"] == _tri_selected]

        # Aggregate across regions for the selected product line
        _tri_agg = (
            _tri_filtered
            .groupby(["accident_month", "dev_lag"])
            .agg({"cumulative_paid": "sum", "cumulative_incurred": "sum", "case_reserve": "sum"})
            .reset_index()
        )

        # Pivot to triangle format: accident_month Ã— dev_lag â†’ cumulative_paid
        if not _tri_agg.empty:
            _pivot = _tri_agg.pivot_table(
                index="accident_month", columns="dev_lag",
                values="cumulative_paid", aggfunc="sum",
            )
            _pivot.columns = [f"Lag {int(c)}" for c in _pivot.columns]
            _pivot.index.name = "Accident Month"

            # Format as currency
            _display_tri = _pivot.map(lambda x: f"${x:,.0f}" if pd.notna(x) else "")
            st.dataframe(_display_tri, use_container_width=True)

            # Development factor summary
            with st.expander("ğŸ“Š Development Factors"):
                st.caption(
                    "Link ratios (cumulative paid at lag N / cumulative paid at lag N-1) â€” "
                    "used by actuaries to estimate ultimate losses via the chain ladder method."
                )
                _dev_factors = {}
                _cols = sorted(_tri_agg["dev_lag"].unique())
                for i in range(len(_cols) - 1):
                    _lag_from = _cols[i]
                    _lag_to = _cols[i + 1]
                    _from_vals = _tri_agg[_tri_agg["dev_lag"] == _lag_from].set_index("accident_month")["cumulative_paid"]
                    _to_vals = _tri_agg[_tri_agg["dev_lag"] == _lag_to].set_index("accident_month")["cumulative_paid"]
                    _common = _from_vals.index.intersection(_to_vals.index)
                    if len(_common) > 0:
                        _ratios = _to_vals[_common] / _from_vals[_common].replace(0, np.nan)
                        _dev_factors[f"{int(_lag_from)}â†’{int(_lag_to)}"] = round(_ratios.mean(), 3)
                if _dev_factors:
                    st.dataframe(
                        pd.DataFrame([_dev_factors], index=["Avg Link Ratio"]),
                        use_container_width=True,
                    )
    else:
        st.info("Reserve triangle data not yet available. Run the DLT pipeline to generate.")
