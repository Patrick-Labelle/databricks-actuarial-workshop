# app.yaml — Databricks App configuration for Actuarial Workshop
#
# DEPLOYMENT
# ──────────
# Via Databricks Asset Bundle (use deploy.sh, not bare bundle deploy):
#      ./deploy.sh --target <target>
#
# Local development:
#      export DATABRICKS_HOST=https://...  CATALOG=...  SCHEMA=...  etc.
#      streamlit run app.py

command:
  - "streamlit"
  - "run"
  - "app.py"
  - "--server.port=8000"
  - "--server.address=0.0.0.0"

env:
  # Streamlit
  - name: STREAMLIT_SERVER_ENABLE_XSRF_PROTECTION
    value: "false"

  # DATABRICKS_HOST, DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET are auto-injected
  # by the Apps runtime for the app's service principal.
  # CATALOG, SCHEMA, PG_DATABASE, ENDPOINT_NAME, LAKEBASE_ENDPOINT_PATH, LAKEBASE_HOST
  # come from app/_bundle_config.py (generated by deploy.sh after bundle deploy +
  # lakebase_setup.py). The app SP is granted CAN_QUERY on the model-serving endpoint
  # by the setup job (src/ops/app_setup.py).

  # SQL execution — injected at runtime from the 'sql-warehouse' app resource.
  # The app's service principal is granted CAN_USE on the warehouse (see app.yml).
  - name: DATABRICKS_WAREHOUSE_ID
    valueFrom: sql-warehouse

  # Lakebase (Postgres) — PGHOST and LAKEBASE_ENDPOINT_PATH come from
  # app/_bundle_config.py (written by deploy.sh). The app authenticates using
  # w.postgres.generate_database_credential(endpoint=LAKEBASE_ENDPOINT_PATH)
  # which returns a short-lived OAuth token used as the Postgres password
  # (Databricks SDK >= 0.81.0, Lakebase Autoscaling pattern).
  # The app SP role is created by scripts/lakebase_setup.py (run by deploy.sh).
  - name: PGPORT
    value: "5432"
  - name: PGSSLMODE
    value: "require"
