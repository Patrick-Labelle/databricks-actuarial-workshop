# ─── Jobs: Actuarial Workshop ─────────────────────────────────────────────────
# Two jobs:
#   1. actuarial_workshop_setup     — one-time full setup (Modules 1–5)
#   2. actuarial_workshop_refresh   — monthly model refresh (DLT + Module 4)

resources:
  jobs:

    # ── Full Setup Job ─────────────────────────────────────────────────────────
    # Run once to build all assets: source data, DLT pipeline, features,
    # statistical models, and the serving endpoint.
    actuarial_workshop_setup:
      name: "Actuarial Workshop — Full Setup"
      description: >-
        One-time setup: generates synthetic data, runs the DLT pipeline,
        builds the Feature Store, fits SARIMA/GARCH models, registers both
        models to UC, and creates the serving endpoints with AI Gateway.

      tasks:

        # Task 1 — Generate synthetic source data (reserve CDC + claim events)
        # Runs notebook 01 in standalone (non-DLT) mode to write reserve_development_raw + claims_events_raw
        - task_key: generate_source_data
          description: "Generate synthetic reserve CDC + claims data → reserve_development_raw, claims_events_raw"
          notebook_task:
            notebook_path: ../src/01_dlt_pipeline_and_jobs.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              notification_email: "${var.notification_email}"
          environment_key: default_env

        # Task 1b — Fetch real macro data from Statistics Canada
        # Runs in parallel with generate_source_data — both handle schema creation
        # independently (CREATE SCHEMA IF NOT EXISTS). Must complete before
        # run_dlt_pipeline so that macro_indicators_raw is populated when the
        # DLT bronze_macro_indicators streaming table runs.
        - task_key: fetch_macro_data
          description: "Fetch StatCan unemployment, HPI, housing starts → macro_indicators_raw"
          notebook_task:
            notebook_path: ../scripts/fetch_macro_data.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
          environment_key: default_env

        # Task 2 — Run the DLT pipeline (Bronze → Silver → Gold)
        # full_refresh: true because generate_source_data recreates source tables via
        # CREATE OR REPLACE TABLE, which invalidates any existing streaming checkpoint.
        # A full refresh clears DLT checkpoints and re-reads the source from scratch.
        # Depends on fetch_macro_data so macro_indicators_raw is populated before
        # bronze_macro_indicators → silver_macro_indicators → gold_macro_features run.
        - task_key: run_dlt_pipeline
          description: "DLT: Bronze → Silver (SCD Type 2) → Gold (reserve triangle + claims + macro)"
          depends_on:
            - task_key: generate_source_data
            - task_key: fetch_macro_data
          pipeline_task:
            pipeline_id: "${resources.pipelines.actuarial_workshop_dlt.id}"
            full_refresh: true

        # Task 3 — Register Feature Store table (Module 3)
        # job_mode=true: writes feature table as plain Delta, skips display() calls,
        # FeatureEngineeringClient, and training set assembly.
        # Online Table creation is handled by Module 5 (prepare_app_infrastructure).
        # These interactive demo sections are preserved for human learners running interactively.
        # Reads silver_rolling_features produced by the DLT pipeline.
        - task_key: build_feature_store
          description: "UC Feature Store registration (Online Table created by Module 5)"
          depends_on:
            - task_key: run_dlt_pipeline
          notebook_task:
            notebook_path: ../src/03_feature_store.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              warehouse_id: "${var.warehouse_id}"
              job_mode: "true"
          environment_key: default_env

        # Task 4 — Fit SARIMAX / GARCH / Monte Carlo + register models (Module 4)
        # Reads gold_claims_monthly (DLT) and gold_macro_features (DLT, populated by fetch_macro_data).
        # run_ray=skip: setup_ray_cluster is not supported on Serverless (Spark Connect
        # does not expose the Ray-on-Spark RPC interface). The single-node NumPy fallback
        # produces identical outputs and is sufficient for the workshop and model registration.
        # job_mode=true: use driver-side pandas groupby.apply() for SARIMAX/GARCH instead of
        # applyInPandas. On Serverless, %pip install only reaches the notebook kernel (client),
        # not the remote compute (server) where applyInPandas UDFs execute. Driver-side fitting
        # produces identical outputs and works reliably on all Serverless runtimes.
        #
        # No environment_key: the notebook handles its own %pip install for job_mode=true.
        # Classic-cluster targets (e2-demo-ray) add job_cluster_key via target override;
        # environment_key cannot coexist with job_cluster_key on classic compute.
        - task_key: fit_statistical_models
          description: "SARIMAX/GARCH/Monte Carlo + register both models (SARIMA + MC) to UC"
          depends_on:
            - task_key: build_feature_store
          notebook_task:
            notebook_path: ../src/04_classical_stats_at_scale.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              run_ray: "skip"
              job_mode: "true"

        # Task 5 — Prepare all app infrastructure (Module 5)
        # Creates both serving endpoints + AI Gateway, the Online Table for
        # low-latency feature lookup, and the Lakebase PostgreSQL database
        # (DB, extension, table, SP role + grants). Uses the Databricks SDK's
        # generate_database_credential() to get a valid JWT for Lakebase auth.
        - task_key: prepare_app_infrastructure
          description: "Serving endpoints + AI Gateway, Online Table, Lakebase DB setup"
          depends_on:
            - task_key: fit_statistical_models
          notebook_task:
            notebook_path: ../src/05_model_serving.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              endpoint_name: "${var.endpoint_name}"
              mc_endpoint_name: "${var.mc_endpoint_name}"
              warehouse_id: "${var.warehouse_id}"
              pg_database: "${var.pg_database}"
              app_sp_client_id: "${resources.apps.actuarial_workshop_app.service_principal_client_id}"
          environment_key: ml_env

        # Task 6 — App setup: UC grants + model serving endpoint grants
        # Runs after all app infrastructure exists so that CAN_QUERY can be granted.
        - task_key: setup_app_dependencies
          description: "Grant USE CATALOG/SCHEMA/SELECT to app SP; grant CAN_QUERY on both serving endpoints"
          depends_on:
            - task_key: prepare_app_infrastructure
          notebook_task:
            notebook_path: ../src/ops/app_setup.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              app_sp_client_id: "${resources.apps.actuarial_workshop_app.service_principal_client_id}"
              endpoint_name: "${var.endpoint_name}"
              mc_endpoint_name: "${var.mc_endpoint_name}"
          environment_key: default_env

      environments:
        - environment_key: default_env
          spec:
            environment_version: "4"
            dependencies:
              - databricks-feature-engineering
        - environment_key: ml_env
          spec:
            environment_version: "4"
            dependencies:
              # statsmodels and arch are available via %pip install in the notebook
              # (for driver-side job_mode fitting) and listed here for the interactive
              # applyInPandas demo path where UDFs run on the remote compute.
              - statsmodels>=0.14
              - arch>=7.0
              - mlflow>=2.14
              - psycopg2-binary
    # ── Monthly Model Refresh Job ──────────────────────────────────────────────
    # Scheduled job: refreshes data via DLT then re-fits the SARIMA models.
    actuarial_workshop_refresh:
      name: "Actuarial Workshop — Monthly Model Refresh"
      description: >-
        Monthly scheduled job: refreshes Bronze→Silver→Gold via DLT,
        then re-fits SARIMA/GARCH models and logs to MLflow.

      schedule:
        quartz_cron_expression: "0 0 3 1 * ?"
        timezone_id: "America/Toronto"
        pause_status: PAUSED

      tasks:

        - task_key: fetch_macro_data
          description: "Fetch latest StatCan macro indicators → macro_indicators_raw"
          notebook_task:
            notebook_path: ../scripts/fetch_macro_data.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"

        - task_key: refresh_dlt_pipeline
          description: "Refresh Bronze→Silver→Gold via DLT Apply Changes (incl. macro SCD2)"
          depends_on:
            - task_key: fetch_macro_data
          pipeline_task:
            pipeline_id: "${resources.pipelines.actuarial_workshop_dlt.id}"
            full_refresh: false

        - task_key: refit_statistical_models
          description: "Re-fit SARIMAX/GARCH models on refreshed data with updated macro signals"
          depends_on:
            - task_key: refresh_dlt_pipeline
          notebook_task:
            notebook_path: ../src/04_classical_stats_at_scale.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              run_ray: "skip"
              job_mode: "true"
          environment_key: ml_env

      environments:
        - environment_key: ml_env
          spec:
            environment_version: "4"
            dependencies:
              - statsmodels>=0.14
              - arch>=7.0
              - mlflow>=2.14
