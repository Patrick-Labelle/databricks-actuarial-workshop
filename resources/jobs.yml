# ─── Jobs: Actuarial Workshop ─────────────────────────────────────────────────
# Two jobs:
#   1. actuarial_workshop_setup     — one-time full setup (Modules 1–5)
#   2. actuarial_workshop_refresh   — monthly model refresh (DLT + Module 4)

resources:
  jobs:

    # ── Full Setup Job ─────────────────────────────────────────────────────────
    # Run once to build all assets: source data, DLT pipeline, features,
    # statistical models, and the serving endpoint.
    actuarial_workshop_setup:
      name: "Actuarial Workshop — Full Setup"
      description: >-
        One-time setup: generates synthetic data, runs the DLT pipeline,
        builds the Feature Store, fits SARIMA/GARCH models, registers the
        model to UC, and creates the serving endpoint.

      tasks:

        # Task 1 — Generate synthetic source data (reserve CDC + claim events)
        # Runs notebook 01 in standalone (non-DLT) mode to write reserve_development_raw + claims_events_raw
        - task_key: generate_source_data
          description: "Generate synthetic reserve CDC + claims data → reserve_development_raw, claims_events_raw"
          notebook_task:
            notebook_path: ../src/01_dlt_pipeline_and_jobs.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              notification_email: "${var.notification_email}"
          environment_key: default_env

        # Task 1b — Fetch real macro data from Statistics Canada
        # Runs in parallel with nothing — depends only on generate_source_data creating
        # the schema. Must complete before run_dlt_pipeline so that macro_indicators_raw
        # exists and is populated when the DLT bronze_macro_indicators streaming table runs.
        - task_key: fetch_macro_data
          description: "Fetch StatCan unemployment, HPI, housing starts → macro_indicators_raw"
          depends_on:
            - task_key: generate_source_data
          notebook_task:
            notebook_path: ../scripts/fetch_macro_data.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
          environment_key: default_env

        # Task 2 — Run the DLT pipeline (Bronze → Silver → Gold)
        # full_refresh: true because generate_source_data recreates source tables via
        # CREATE OR REPLACE TABLE, which invalidates any existing streaming checkpoint.
        # A full refresh clears DLT checkpoints and re-reads the source from scratch.
        # Depends on fetch_macro_data so macro_indicators_raw is populated before
        # bronze_macro_indicators → silver_macro_indicators → gold_macro_features run.
        - task_key: run_dlt_pipeline
          description: "DLT: Bronze → Silver (SCD Type 2) → Gold (reserve triangle + claims + macro)"
          depends_on:
            - task_key: generate_source_data
            - task_key: fetch_macro_data
          pipeline_task:
            pipeline_id: "${resources.pipelines.actuarial_workshop_dlt.id}"
            full_refresh: true

        # Task 3 — Build rolling features (Module 2)
        - task_key: build_rolling_features
          description: "Compute rolling means and volatility features per segment"
          depends_on:
            - task_key: run_dlt_pipeline
          notebook_task:
            notebook_path: ../src/02_spark_vs_ray.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              run_ray: "skip"   # Ray on Spark not supported on serverless; interactive demo only
          environment_key: default_env

        # Task 4 — Register Feature Store table and Online Table (Module 3)
        # job_mode=true: writes feature table as plain Delta, skips display() calls,
        # FeatureEngineeringClient, training set assembly, and Online Table creation.
        # These interactive demo sections are preserved for human learners running interactively.
        - task_key: build_feature_store
          description: "UC Feature Store registration + Online Table for low-latency serving"
          depends_on:
            - task_key: build_rolling_features
          notebook_task:
            notebook_path: ../src/03_feature_store.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              warehouse_id: "${var.warehouse_id}"
              job_mode: "true"
          environment_key: default_env

        # Task 5 — Fit SARIMAX / GARCH / Monte Carlo (Module 4)
        # Reads gold_claims_monthly (DLT) and gold_macro_features (DLT, populated by fetch_macro_data).
        # run_ray=skip: setup_ray_cluster is not supported on Serverless (Spark Connect
        # does not expose the Ray-on-Spark RPC interface). The single-node NumPy fallback
        # produces identical outputs and is sufficient for the workshop and model registration.
        # job_mode=true: use driver-side pandas groupby.apply() for SARIMAX/GARCH instead of
        # applyInPandas. On Serverless, %pip install only reaches the notebook kernel (client),
        # not the remote compute (server) where applyInPandas UDFs execute. Driver-side fitting
        # produces identical outputs and works reliably on all Serverless runtimes.
        #
        # No environment_key: the notebook handles its own %pip install for job_mode=true.
        # Classic-cluster targets (e2-demo-ray) add job_cluster_key via target override;
        # environment_key cannot coexist with job_cluster_key on classic compute.
        - task_key: fit_statistical_models
          description: "SARIMAX per segment (+ macro exog), GARCH volatility, Monte Carlo simulation"
          depends_on:
            - task_key: build_feature_store
          notebook_task:
            notebook_path: ../src/04_classical_stats_at_scale.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              run_ray: "skip"
              job_mode: "true"

        # Task 6a — Register SARIMA model + create serving endpoint (Module 5)
        - task_key: register_and_serve_model
          description: "MLflow PyFunc, UC Model Registry @Champion, Model Serving endpoint"
          depends_on:
            - task_key: fit_statistical_models
          notebook_task:
            notebook_path: ../src/05_mlflow_uc_serving.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              endpoint_name: "${var.endpoint_name}"
          environment_key: ml_env

        # Task 6b — Register Monte Carlo model + create CPU serving endpoint (Module 6)
        # Runs in parallel with task 6a: both depend on fit_statistical_models.
        # CPU endpoint is sufficient — scipy t-CDF is not GPU-accelerated.
        - task_key: register_monte_carlo_endpoint
          description: "t-Copula PyFunc, UC Model Registry @Champion, CPU Monte Carlo endpoint"
          depends_on:
            - task_key: fit_statistical_models
          notebook_task:
            notebook_path: ../src/06_monte_carlo_serving.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              mc_endpoint_name: "${var.mc_endpoint_name}"
          environment_key: ml_env

        # Task 7 — App setup: UC grants + model serving endpoint grants
        # Runs after both serving endpoints exist so that CAN_QUERY can be granted.
        #
        # Lakebase PostgreSQL setup (create DB, extension, SP role, table, grants)
        # is handled by scripts/lakebase_setup.py running locally from deploy.sh,
        # using the CLI's OAuth JWT. Internal cluster tokens cannot authenticate
        # with Lakebase Autoscaling's databricks_auth extension.
        - task_key: setup_app_dependencies
          description: "Grant USE CATALOG/SCHEMA/SELECT to app SP; grant CAN_QUERY on both serving endpoints"
          depends_on:
            - task_key: register_and_serve_model
            - task_key: register_monte_carlo_endpoint
          notebook_task:
            notebook_path: ../src/ops/app_setup.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              app_sp_client_id: "${resources.apps.actuarial_workshop_app.service_principal_client_id}"
              endpoint_name: "${var.endpoint_name}"
              mc_endpoint_name: "${var.mc_endpoint_name}"
          environment_key: default_env

      environments:
        - environment_key: default_env
          spec:
            environment_version: "4"
            dependencies:
              - databricks-feature-engineering
        - environment_key: ml_env
          spec:
            environment_version: "4"
            dependencies:
              # statsmodels and arch are available via %pip install in the notebook
              # (for driver-side job_mode fitting) and listed here for the interactive
              # applyInPandas demo path where UDFs run on the remote compute.
              - statsmodels>=0.14
              - arch>=7.0
              - mlflow>=2.14
    # ── Monthly Model Refresh Job ──────────────────────────────────────────────
    # Scheduled job: refreshes data via DLT then re-fits the SARIMA models.
    actuarial_workshop_refresh:
      name: "Actuarial Workshop — Monthly Model Refresh"
      description: >-
        Monthly scheduled job: refreshes Bronze→Silver→Gold via DLT,
        then re-fits SARIMA/GARCH models and logs to MLflow.

      schedule:
        quartz_cron_expression: "0 0 3 1 * ?"
        timezone_id: "America/Toronto"
        pause_status: PAUSED

      tasks:

        - task_key: fetch_macro_data
          description: "Fetch latest StatCan macro indicators → macro_indicators_raw"
          notebook_task:
            notebook_path: ../scripts/fetch_macro_data.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"

        - task_key: refresh_dlt_pipeline
          description: "Refresh Bronze→Silver→Gold via DLT Apply Changes (incl. macro SCD2)"
          depends_on:
            - task_key: fetch_macro_data
          pipeline_task:
            pipeline_id: "${resources.pipelines.actuarial_workshop_dlt.id}"
            full_refresh: false

        - task_key: refit_statistical_models
          description: "Re-fit SARIMAX/GARCH models on refreshed data with updated macro signals"
          depends_on:
            - task_key: refresh_dlt_pipeline
          notebook_task:
            notebook_path: ../src/04_classical_stats_at_scale.py
            base_parameters:
              catalog: "${var.catalog}"
              schema: "${var.schema}"
              run_ray: "skip"
              job_mode: "true"
          environment_key: ml_env

      environments:
        - environment_key: ml_env
          spec:
            environment_version: "4"
            dependencies:
              - statsmodels>=0.14
              - arch>=7.0
              - mlflow>=2.14
