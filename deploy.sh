#!/usr/bin/env bash
# deploy.sh — full end-to-end deploy: bundle + setup job + app
#
# Usage:  ./deploy.sh [--target <target>] [other bundle flags]
#
# Sequence
# --------
#   1. Resolve bundle variables via `bundle validate`; generate app/_bundle_config.py
#      (app/app.yaml is uploaded as source and does not receive DAB variable
#      substitution, so catalog/schema/pg_database are injected via this file).
#   2. `databricks bundle deploy` — provisions all bundle-managed resources
#      (Lakebase instance, jobs, DLT pipeline, app resource).
#   3. If a fresh app was created with compute STOPPED, call `apps start` to
#      start the compute and clear the bundle's initial-deployment lock.
#   4. `databricks bundle run actuarial_workshop_setup` — runs the setup job,
#      which seeds the data, trains and registers the SARIMA model, and (crucially)
#      grants the app's service principal permissions on all UC catalog/schema/table
#      assets, the Lakebase PostgreSQL database, and the model-serving endpoint.
#   5. `databricks apps deploy` — performs the final app deployment only after
#      all permissions are in place, so the app starts cleanly without
#      catalog-permission errors.
#
# app/_bundle_config.py is gitignored and re-generated on every deploy.

set -euo pipefail

# ── Minimum CLI version check ─────────────────────────────────────────────────
# postgres_projects/branches/endpoints resources require Databricks CLI >= 0.287.0.
_CLI_VER=$(databricks --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "0.0.0")
_CLI_MAJOR=$(echo "$_CLI_VER" | cut -d. -f1)
_CLI_MINOR=$(echo "$_CLI_VER" | cut -d. -f2)
if [ "$_CLI_MAJOR" -lt 1 ] && [ "$_CLI_MINOR" -lt 287 ]; then
    echo "ERROR: Databricks CLI >= 0.287.0 required (found ${_CLI_VER})." >&2
    echo "       Install: https://docs.databricks.com/aws/en/dev-tools/cli/install" >&2
    exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BUNDLE_ARGS=("$@")

# ── Step 1: resolve variables and generate app/_bundle_config.py ─────────────
echo "==> Resolving bundle variables..."
VALIDATE_JSON=$(databricks bundle validate --output json "${BUNDLE_ARGS[@]}" 2>/dev/null)

CATALOG=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('catalog',{}).get('value','my_catalog'))")
SCHEMA=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('schema',{}).get('value','actuarial_workshop'))")
PG_DATABASE=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('pg_database',{}).get('value','actuarial_workshop_db'))")
ENDPOINT_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('endpoint_name',{}).get('value','actuarial-workshop-sarima-forecaster'))")
MC_ENDPOINT_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('mc_endpoint_name',{}).get('value','actuarial-workshop-monte-carlo'))")

echo "==> Generating app/_bundle_config.py (initial — LAKEBASE_HOST added after lakebase setup)"
echo "    CATALOG=${CATALOG}, SCHEMA=${SCHEMA}, PG_DATABASE=${PG_DATABASE}, ENDPOINT_NAME=${ENDPOINT_NAME}, MC_ENDPOINT_NAME=${MC_ENDPOINT_NAME}"
# LAKEBASE_ENDPOINT_PATH is the resource path used by the SDK's
# w.postgres.generate_database_credential(endpoint=...) call.
# Format: projects/{project_id}/branches/{branch_id}/endpoints/{endpoint_id}
# These IDs match the resource definitions in resources/lakebase.yml.
LAKEBASE_ENDPOINT_PATH="projects/actuarial-workshop-lakebase/branches/main/endpoints/primary"
# LAKEBASE_HOST is written below (after lakebase_setup.py resolves it) — placeholder for now.
cat > "${SCRIPT_DIR}/app/_bundle_config.py" << EOF
# Auto-generated by deploy.sh — do not edit or commit
CATALOG = '${CATALOG}'
SCHEMA = '${SCHEMA}'
PG_DATABASE = '${PG_DATABASE}'
ENDPOINT_NAME = '${ENDPOINT_NAME}'
MC_ENDPOINT_NAME = '${MC_ENDPOINT_NAME}'
LAKEBASE_ENDPOINT_PATH = '${LAKEBASE_ENDPOINT_PATH}'
LAKEBASE_HOST = ''
EOF

# Extract app name, workspace source path, and CLI profile from validate output.
APP_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "
import sys, json
d = json.load(sys.stdin)
apps = d.get('resources', {}).get('apps', {})
print(list(apps.values())[0].get('name', '')) if apps else print('')
")
APP_SOURCE_PATH=$(echo "$VALIDATE_JSON" \
    | python3 -c "
import sys, json
d = json.load(sys.stdin)
apps = d.get('resources', {}).get('apps', {})
print(list(apps.values())[0].get('source_code_path', '')) if apps else print('')
")
PROFILE=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('workspace',{}).get('profile',''))")

PROFILE_ARGS=()
if [ -n "$PROFILE" ]; then
    PROFILE_ARGS=(--profile "$PROFILE")
fi

WORKSPACE_HOST=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('workspace',{}).get('host',''))")

# ── Step 2: bundle deploy ─────────────────────────────────────────────────────
echo "==> Running databricks bundle deploy ${BUNDLE_ARGS[*]}"
databricks bundle deploy "${BUNDLE_ARGS[@]}"

# ── Step 2.5: Lakebase database setup (runs locally using CLI OAuth JWT) ──────
# Lakebase Autoscaling endpoints require standard OAuth JWTs for authentication.
# Internal Databricks cluster tokens (apiToken(), DATABRICKS_TOKEN on job compute)
# are opaque credentials rejected by Lakebase's databricks_auth extension.
# The local CLI provides a proper OAuth JWT, so we run setup here instead of
# from within the notebook.
if python3 -c "import psycopg2" 2>/dev/null; then
    # Get app SP client ID from the deployed app resource
    SP_CLIENT_ID=""
    if [ -n "$APP_NAME" ]; then
        SP_CLIENT_ID=$(databricks api get "/api/2.0/apps/${APP_NAME}" \
            "${PROFILE_ARGS[@]}" 2>/dev/null \
            | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    d = json.loads('\n'.join(lines[j:]))
    print(d.get('service_principal_client_id', ''))
" 2>/dev/null || echo "")
    fi

    echo "==> Setting up Lakebase database (using local OAuth JWT)..."
    echo "    App SP client ID: ${SP_CLIENT_ID:-(not found)}"
    LAKEBASE_HOST_FILE=$(mktemp)
    python3 "${SCRIPT_DIR}/scripts/lakebase_setup.py" \
        --workspace-host "${WORKSPACE_HOST}" \
        --endpoint-path  "${LAKEBASE_ENDPOINT_PATH}" \
        --pg-database    "${PG_DATABASE}" \
        --app-sp-client-id "${SP_CLIENT_ID}" \
        --output-host-file "${LAKEBASE_HOST_FILE}" \
        ${PROFILE:+--profile "$PROFILE"}
    LAKEBASE_HOST=$(cat "${LAKEBASE_HOST_FILE}" 2>/dev/null || echo "")
    rm -f "${LAKEBASE_HOST_FILE}"

    # Regenerate _bundle_config.py with the now-known Lakebase hostname.
    # The app reads LAKEBASE_HOST from this file so it doesn't need to call
    # the endpoint status API at runtime (which the SP may not have permission for).
    cat > "${SCRIPT_DIR}/app/_bundle_config.py" << EOF
# Auto-generated by deploy.sh — do not edit or commit
CATALOG = '${CATALOG}'
SCHEMA = '${SCHEMA}'
PG_DATABASE = '${PG_DATABASE}'
ENDPOINT_NAME = '${ENDPOINT_NAME}'
MC_ENDPOINT_NAME = '${MC_ENDPOINT_NAME}'
LAKEBASE_ENDPOINT_PATH = '${LAKEBASE_ENDPOINT_PATH}'
LAKEBASE_HOST = '${LAKEBASE_HOST}'
EOF
    echo "    LAKEBASE_HOST=${LAKEBASE_HOST}"
else
    echo "WARNING: psycopg2-binary not installed — skipping Lakebase setup."
    echo "         Install with: pip install psycopg2-binary"
    echo "         Then re-run deploy.sh to complete Lakebase setup."
fi

# ── Step 3: start app compute if needed ──────────────────────────────────────
# When bundle creates a fresh app it leaves compute STOPPED and queues an
# initial deployment internally. That deployment holds the deployment lock,
# permanently blocking our later `apps deploy` call. Starting the compute
# clears the lock without deploying anything, so step 5 can proceed cleanly.
if [ -n "$APP_NAME" ]; then
    COMPUTE_STATE=$(databricks api get "/api/2.0/apps/${APP_NAME}" \
        "${PROFILE_ARGS[@]}" 2>/dev/null \
        | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    print(json.loads('\n'.join(lines[j:])).get('compute_status', {}).get('state', ''))
" 2>/dev/null || echo "")
    if [ "$COMPUTE_STATE" = "STOPPED" ]; then
        echo "==> App compute is STOPPED — starting it to clear bundle's initial deployment lock..."
        databricks apps start "${APP_NAME}" "${PROFILE_ARGS[@]}" > /dev/null 2>&1 || true
        echo "    App compute started."
    fi
fi

# ── Step 4: run setup job ─────────────────────────────────────────────────────
# This provisions all data assets and — critically — grants the app's service
# principal permissions on UC catalog/schema/tables, the Lakebase PostgreSQL
# database, and the model-serving endpoint. The app must not be deployed until
# this completes, otherwise it starts with missing permissions.
#
# Note: `databricks bundle run` streams job output and can fail with
# "Invalid Token" if the OAuth token expires during a long job (>60 min).
# The job itself continues running on Databricks; we detect this case and
# poll directly with `jobs get-run` until the job reaches a terminal state.
echo "==> Running setup job (this takes ~15 min)..."

# Extract the setup job ID from the deployed bundle state for polling fallback.
# (The job ID is only assigned after bundle deploy, so we query it from the API
# using --name filter — much faster than listing all jobs.)
SETUP_JOB_ID=$(databricks jobs list \
    --name "Actuarial Workshop — Full Setup" \
    --output json \
    "${PROFILE_ARGS[@]}" 2>/dev/null \
    | python3 -c "
import sys, json
data = json.load(sys.stdin)
jobs = data if isinstance(data, list) else data.get('jobs', [])
if jobs:
    print(jobs[0].get('job_id', ''))
" 2>/dev/null || echo "")

_BUNDLE_RUN_OK=true
databricks bundle run actuarial_workshop_setup "${BUNDLE_ARGS[@]}" || _BUNDLE_RUN_OK=false

if [ "$_BUNDLE_RUN_OK" = "false" ]; then
    echo "WARNING: 'bundle run' exited with an error (possibly OAuth token expiry mid-stream)."
    echo "         Checking whether the setup job actually completed successfully..."

    if [ -z "$SETUP_JOB_ID" ]; then
        echo "ERROR: Could not determine setup job ID — cannot verify job outcome." >&2
        exit 1
    fi

    # Poll the most recent run until it reaches a terminal state.
    _POLL_MAX=120   # max 120 × 60s = 2 hours
    _POLL_COUNT=0
    _FINAL_RESULT=""
    while [ $_POLL_COUNT -lt $_POLL_MAX ]; do
        _RUN_STATE=$(databricks api get \
            "/api/2.1/jobs/runs/list?job_id=${SETUP_JOB_ID}&limit=1" \
            "${PROFILE_ARGS[@]}" 2>/dev/null \
            | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    runs = json.loads('\n'.join(lines[j:])).get('runs', [])
    if runs:
        s = runs[0].get('state', {})
        print(s.get('life_cycle_state',''), s.get('result_state',''))
" 2>/dev/null || echo "UNKNOWN ")
        _LC=$(echo "$_RUN_STATE" | awk '{print $1}')
        _RS=$(echo "$_RUN_STATE" | awk '{print $2}')
        if [ "$_LC" = "TERMINATED" ] || [ "$_LC" = "INTERNAL_ERROR" ]; then
            _FINAL_RESULT="$_RS"
            break
        fi
        echo "    Job still running (${_LC}) — waiting 60s... (poll ${_POLL_COUNT}/${_POLL_MAX})"
        sleep 60
        _POLL_COUNT=$((_POLL_COUNT + 1))
    done

    if [ "$_FINAL_RESULT" = "SUCCESS" ]; then
        echo "    Job completed successfully despite bundle run auth error. Continuing to app deploy."
    else
        echo "ERROR: Setup job did not complete successfully (result=${_FINAL_RESULT:-TIMEOUT})." >&2
        exit 1
    fi
fi

# ── Step 5: deploy app ────────────────────────────────────────────────────────
# All permissions are now in place. Deploy the app source code so it starts
# with a clean slate against the fully-provisioned, permissioned environment.
if [ -n "$APP_NAME" ] && [ -n "$APP_SOURCE_PATH" ]; then
    echo "==> Deploying app source code..."
    echo "    App:    ${APP_NAME}"
    echo "    Source: ${APP_SOURCE_PATH}"
    databricks apps deploy "${APP_NAME}" \
        --source-code-path "${APP_SOURCE_PATH}" \
        "${PROFILE_ARGS[@]}"
    echo "==> App deployment complete!"
else
    echo "==> No app found in bundle, skipping app deployment."
fi
