#!/usr/bin/env bash
# deploy.sh — full end-to-end deploy: bundle + setup job + app
#
# Usage:  ./deploy.sh [--target <target>] [other bundle flags]
#
# Sequence
# --------
#   1. Resolve bundle variables via `bundle validate`; generate app/_bundle_config.py
#      (app/app.yaml is uploaded as source and does not receive DAB variable
#      substitution, so catalog/schema/pg_database are injected via this file).
#   2. `databricks bundle deploy` — provisions all bundle-managed resources
#      (Lakebase instance, jobs, DLT pipeline, app resource).
#   3. If a fresh app was created with compute STOPPED, call `apps start` to
#      start the compute and clear the bundle's initial-deployment lock.
#   4. `databricks bundle run actuarial_workshop_setup` — runs the setup job,
#      which seeds the data, trains and registers the SARIMA model, and (crucially)
#      grants the app's service principal permissions on all UC catalog/schema/table
#      assets, the Lakebase PostgreSQL database, and the model-serving endpoint.
#   5. `databricks apps deploy` — performs the final app deployment only after
#      all permissions are in place, so the app starts cleanly without
#      catalog-permission errors.
#
# app/_bundle_config.py is gitignored and re-generated on every deploy.

set -euo pipefail

# ── Minimum CLI version check ─────────────────────────────────────────────────
# postgres_projects/branches/endpoints resources require Databricks CLI >= 0.287.0.
_CLI_VER=$(databricks --version 2>/dev/null | grep -oE '[0-9]+\.[0-9]+\.[0-9]+' | head -1 || echo "0.0.0")
_CLI_MAJOR=$(echo "$_CLI_VER" | cut -d. -f1)
_CLI_MINOR=$(echo "$_CLI_VER" | cut -d. -f2)
if [ "$_CLI_MAJOR" -lt 1 ] && [ "$_CLI_MINOR" -lt 287 ]; then
    echo "ERROR: Databricks CLI >= 0.287.0 required (found ${_CLI_VER})." >&2
    echo "       Install: https://docs.databricks.com/aws/en/dev-tools/cli/install" >&2
    exit 1
fi

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
BUNDLE_ARGS=("$@")

# ── Step 1: resolve variables and generate app/_bundle_config.py ─────────────
echo "==> Resolving bundle variables..."
VALIDATE_JSON=$(databricks bundle validate --output json "${BUNDLE_ARGS[@]}" 2>/dev/null)

CATALOG=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('catalog',{}).get('value','my_catalog'))")
SCHEMA=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('schema',{}).get('value','actuarial_workshop'))")
PG_DATABASE=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('pg_database',{}).get('value','actuarial_workshop_db'))")
ENDPOINT_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('endpoint_name',{}).get('value','actuarial-workshop-sarima-forecaster'))")
MC_ENDPOINT_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('variables',{}).get('mc_endpoint_name',{}).get('value','actuarial-workshop-monte-carlo'))")

echo "==> Generating app/_bundle_config.py (initial — LAKEBASE_HOST added after lakebase setup)"
echo "    CATALOG=${CATALOG}, SCHEMA=${SCHEMA}, PG_DATABASE=${PG_DATABASE}, ENDPOINT_NAME=${ENDPOINT_NAME}, MC_ENDPOINT_NAME=${MC_ENDPOINT_NAME}"
# LAKEBASE_ENDPOINT_PATH is the resource path used by the SDK's
# w.postgres.generate_database_credential(endpoint=...) call.
# Format: projects/{project_id}/branches/{branch_id}/endpoints/{endpoint_id}
# These IDs match the resource definitions in resources/lakebase.yml.
LAKEBASE_ENDPOINT_PATH="projects/actuarial-workshop-lakebase/branches/main/endpoints/primary"
# LAKEBASE_HOST is written below (after lakebase_setup.py resolves it) — placeholder for now.
cat > "${SCRIPT_DIR}/app/_bundle_config.py" << EOF
# Auto-generated by deploy.sh — do not edit or commit
CATALOG = '${CATALOG}'
SCHEMA = '${SCHEMA}'
PG_DATABASE = '${PG_DATABASE}'
ENDPOINT_NAME = '${ENDPOINT_NAME}'
MC_ENDPOINT_NAME = '${MC_ENDPOINT_NAME}'
LAKEBASE_ENDPOINT_PATH = '${LAKEBASE_ENDPOINT_PATH}'
LAKEBASE_HOST = ''
EOF

# Extract app name, workspace source path, and CLI profile from validate output.
APP_NAME=$(echo "$VALIDATE_JSON" \
    | python3 -c "
import sys, json
d = json.load(sys.stdin)
apps = d.get('resources', {}).get('apps', {})
print(list(apps.values())[0].get('name', '')) if apps else print('')
")
APP_SOURCE_PATH=$(echo "$VALIDATE_JSON" \
    | python3 -c "
import sys, json
d = json.load(sys.stdin)
apps = d.get('resources', {}).get('apps', {})
print(list(apps.values())[0].get('source_code_path', '')) if apps else print('')
")
PROFILE=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('workspace',{}).get('profile',''))")

PROFILE_ARGS=()
if [ -n "$PROFILE" ]; then
    PROFILE_ARGS=(--profile "$PROFILE")
fi

WORKSPACE_HOST=$(echo "$VALIDATE_JSON" \
    | python3 -c "import sys,json; d=json.load(sys.stdin); print(d.get('workspace',{}).get('host',''))")

# ── Step 2: bundle deploy ─────────────────────────────────────────────────────
echo "==> Running databricks bundle deploy ${BUNDLE_ARGS[*]}"
databricks bundle deploy "${BUNDLE_ARGS[@]}"

# ── Step 2.5: Lakebase database setup (runs locally using CLI OAuth JWT) ──────
# Lakebase Autoscaling endpoints require standard OAuth JWTs for authentication.
# Internal Databricks cluster tokens (apiToken(), DATABRICKS_TOKEN on job compute)
# are opaque credentials rejected by Lakebase's databricks_auth extension.
# The local CLI provides a proper OAuth JWT, so we run setup here instead of
# from within the notebook.
if python3 -c "import psycopg2" 2>/dev/null; then
    # Get app SP client ID from the deployed app resource
    SP_CLIENT_ID=""
    if [ -n "$APP_NAME" ]; then
        SP_CLIENT_ID=$(databricks api get "/api/2.0/apps/${APP_NAME}" \
            "${PROFILE_ARGS[@]}" 2>/dev/null \
            | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    d = json.loads('\n'.join(lines[j:]))
    print(d.get('service_principal_client_id', ''))
" 2>/dev/null || echo "")
    fi

    echo "==> Setting up Lakebase database (using local OAuth JWT)..."
    echo "    App SP client ID: ${SP_CLIENT_ID:-(not found)}"
    LAKEBASE_HOST_FILE=$(mktemp)
    python3 "${SCRIPT_DIR}/scripts/lakebase_setup.py" \
        --workspace-host "${WORKSPACE_HOST}" \
        --endpoint-path  "${LAKEBASE_ENDPOINT_PATH}" \
        --pg-database    "${PG_DATABASE}" \
        --app-sp-client-id "${SP_CLIENT_ID}" \
        --output-host-file "${LAKEBASE_HOST_FILE}" \
        ${PROFILE:+--profile "$PROFILE"}
    LAKEBASE_HOST=$(cat "${LAKEBASE_HOST_FILE}" 2>/dev/null || echo "")
    rm -f "${LAKEBASE_HOST_FILE}"

    # Regenerate _bundle_config.py with the now-known Lakebase hostname.
    # The app reads LAKEBASE_HOST from this file so it doesn't need to call
    # the endpoint status API at runtime (which the SP may not have permission for).
    cat > "${SCRIPT_DIR}/app/_bundle_config.py" << EOF
# Auto-generated by deploy.sh — do not edit or commit
CATALOG = '${CATALOG}'
SCHEMA = '${SCHEMA}'
PG_DATABASE = '${PG_DATABASE}'
ENDPOINT_NAME = '${ENDPOINT_NAME}'
MC_ENDPOINT_NAME = '${MC_ENDPOINT_NAME}'
LAKEBASE_ENDPOINT_PATH = '${LAKEBASE_ENDPOINT_PATH}'
LAKEBASE_HOST = '${LAKEBASE_HOST}'
EOF
    echo "    LAKEBASE_HOST=${LAKEBASE_HOST}"
else
    echo "WARNING: psycopg2-binary not installed — skipping Lakebase setup."
    echo "         Install with: pip install psycopg2-binary"
    echo "         Then re-run deploy.sh to complete Lakebase setup."
fi

# ── Step 3: start app compute if needed ──────────────────────────────────────
# When bundle creates a fresh app it leaves compute STOPPED and queues an
# initial deployment internally. That deployment holds the deployment lock,
# permanently blocking our later `apps deploy` call. Starting the compute
# clears the lock without deploying anything, so step 5 can proceed cleanly.
if [ -n "$APP_NAME" ]; then
    COMPUTE_STATE=$(databricks api get "/api/2.0/apps/${APP_NAME}" \
        "${PROFILE_ARGS[@]}" 2>/dev/null \
        | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    print(json.loads('\n'.join(lines[j:])).get('compute_status', {}).get('state', ''))
" 2>/dev/null || echo "")
    if [ "$COMPUTE_STATE" = "STOPPED" ]; then
        echo "==> App compute is STOPPED — starting it to clear bundle's initial deployment lock..."
        databricks apps start "${APP_NAME}" "${PROFILE_ARGS[@]}" > /dev/null 2>&1 || true
        echo "    App compute started."
    fi
fi

# ── Step 4: run setup job ─────────────────────────────────────────────────────
# This provisions all data assets and — critically — grants the app's service
# principal permissions on UC catalog/schema/tables, the Lakebase PostgreSQL
# database, and the model-serving endpoints.
#
# Deploy strategy: the app source code is deployed as soon as the model serving
# endpoints are ready (tasks register_and_serve_model + register_monte_carlo_endpoint
# both succeed), rather than waiting for the full job to finish.  The final task
# (setup_app_dependencies, ~21s) grants UC and CAN_QUERY permissions; by the time
# the app compute starts, those grants are in place.
echo "==> Running setup job (this takes ~15 min)..."

# Resolve the setup job ID (needed for async polling and fallback).
SETUP_JOB_ID=$(databricks jobs list \
    --name "Actuarial Workshop — Full Setup" \
    --output json \
    "${PROFILE_ARGS[@]}" 2>/dev/null \
    | python3 -c "
import sys, json
data = json.load(sys.stdin)
jobs = data if isinstance(data, list) else data.get('jobs', [])
if jobs:
    print(jobs[0].get('job_id', ''))
" 2>/dev/null || echo "")

# ── 4a: Launch the job and capture the run ID ─────────────────────────────────
_BUNDLE_RUN_OK=true
databricks bundle run actuarial_workshop_setup "${BUNDLE_ARGS[@]}" &
_BUNDLE_RUN_PID=$!

# Resolve the latest run ID shortly after launch (give it a few seconds to register).
sleep 10
SETUP_RUN_ID=""
if [ -n "$SETUP_JOB_ID" ]; then
    SETUP_RUN_ID=$(databricks api get \
        "/api/2.1/jobs/runs/list?job_id=${SETUP_JOB_ID}&limit=1" \
        "${PROFILE_ARGS[@]}" 2>/dev/null \
        | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    runs = json.loads('\n'.join(lines[j:])).get('runs', [])
    if runs:
        print(runs[0].get('run_id', ''))
" 2>/dev/null || echo "")
fi
echo "    Run ID: ${SETUP_RUN_ID:-unknown}"

# ── 4b: Poll task states; deploy app once endpoints are ready ─────────────────
_APP_DEPLOYED=false
_POLL_MAX=180   # max 180 × 20s = 1 hour
_POLL_COUNT=0
_FINAL_RESULT=""

while [ $_POLL_COUNT -lt $_POLL_MAX ]; do
    sleep 20

    # Fetch the current task states for this run
    if [ -z "$SETUP_RUN_ID" ]; then
        # Fall back to checking the most recent run
        _RUN_DATA=$(databricks api get \
            "/api/2.1/jobs/runs/list?job_id=${SETUP_JOB_ID}&limit=1" \
            "${PROFILE_ARGS[@]}" 2>/dev/null)
        SETUP_RUN_ID=$(echo "$_RUN_DATA" | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    runs = json.loads('\n'.join(lines[j:])).get('runs', [])
    if runs: print(runs[0].get('run_id', ''))
" 2>/dev/null || echo "")
    fi

    _TASK_STATES=$(databricks api get \
        "/api/2.1/jobs/runs/get?run_id=${SETUP_RUN_ID}" \
        "${PROFILE_ARGS[@]}" 2>/dev/null \
        | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is None: sys.exit()
data = json.loads('\n'.join(lines[j:]))
run_state = data.get('state', {})
lc = run_state.get('life_cycle_state','')
rs = run_state.get('result_state','')
print(f'RUN {lc} {rs}')
for t in data.get('tasks', []):
    s = t.get('state', {})
    print(t['task_key'], s.get('life_cycle_state','PENDING'), s.get('result_state',''))
" 2>/dev/null || echo "RUN UNKNOWN ")

    # Parse overall run state
    _RUN_LC=$(echo "$_TASK_STATES" | awk 'NR==1{print $2}')
    _RUN_RS=$(echo "$_TASK_STATES" | awk 'NR==1{print $3}')

    # Check if the run reached a terminal state
    if [ "$_RUN_LC" = "TERMINATED" ] || [ "$_RUN_LC" = "INTERNAL_ERROR" ]; then
        _FINAL_RESULT="$_RUN_RS"
        break
    fi

    # Check endpoint tasks: deploy app once both serving endpoint tasks succeed
    if [ "$_APP_DEPLOYED" = "false" ] && [ -n "$APP_NAME" ] && [ -n "$APP_SOURCE_PATH" ]; then
        _SERVE_SARIMA=$(echo "$_TASK_STATES" | awk '$1=="register_and_serve_model"{print $2, $3}')
        _SERVE_MC=$(echo "$_TASK_STATES"    | awk '$1=="register_monte_carlo_endpoint"{print $2, $3}')
        _SARIMA_DONE=$(echo "$_SERVE_SARIMA" | grep -c "TERMINATED SUCCESS" || true)
        _MC_DONE=$(echo "$_SERVE_MC"         | grep -c "TERMINATED SUCCESS" || true)

        if [ "$_SARIMA_DONE" -ge 1 ] && [ "$_MC_DONE" -ge 1 ]; then
            echo ""
            echo "==> Model serving endpoints ready — deploying app source code..."
            echo "    (setup_app_dependencies will finish granting permissions in ~21s)"
            databricks apps deploy "${APP_NAME}" \
                --source-code-path "${APP_SOURCE_PATH}" \
                "${PROFILE_ARGS[@]}" && _APP_DEPLOYED=true || true
            echo "==> App source deployed — waiting for permission grants to complete..."
        fi
    fi

    # Progress indicator
    _RUNNING=$(echo "$_TASK_STATES" | awk '$2=="RUNNING"{print $1}' | tr '\n' ' ')
    _DONE=$(echo "$_TASK_STATES" | grep -c "TERMINATED SUCCESS" || true)
    echo "    [${_DONE}/8 tasks done] running: ${_RUNNING:-none} (poll ${_POLL_COUNT}/${_POLL_MAX})"
    _POLL_COUNT=$((_POLL_COUNT + 1))
done

# Wait for the background bundle run process to exit cleanly
wait $_BUNDLE_RUN_PID 2>/dev/null || _BUNDLE_RUN_OK=false

if [ -z "$_FINAL_RESULT" ]; then
    # Polling loop hit max without a terminal state
    echo "WARNING: Polling timed out — checking final job state..."
    _FINAL_RESULT=$(databricks api get \
        "/api/2.1/jobs/runs/list?job_id=${SETUP_JOB_ID}&limit=1" \
        "${PROFILE_ARGS[@]}" 2>/dev/null \
        | python3 -c "
import sys, json
lines = sys.stdin.read().split('\n')
j = next((i for i,l in enumerate(lines) if l.strip().startswith('{')), None)
if j is not None:
    runs = json.loads('\n'.join(lines[j:])).get('runs', [])
    if runs:
        s = runs[0].get('state', {})
        print(s.get('result_state',''))
" 2>/dev/null || echo "")
fi

if [ "$_FINAL_RESULT" != "SUCCESS" ]; then
    echo "ERROR: Setup job did not complete successfully (result=${_FINAL_RESULT:-UNKNOWN})." >&2
    exit 1
fi
echo "==> Setup job complete!"

# ── Step 5: deploy app (if not already deployed during step 4) ────────────────
if [ "$_APP_DEPLOYED" = "false" ] && [ -n "$APP_NAME" ] && [ -n "$APP_SOURCE_PATH" ]; then
    echo "==> Deploying app source code (post-job — all permissions in place)..."
    echo "    App:    ${APP_NAME}"
    echo "    Source: ${APP_SOURCE_PATH}"
    databricks apps deploy "${APP_NAME}" \
        --source-code-path "${APP_SOURCE_PATH}" \
        "${PROFILE_ARGS[@]}"
    echo "==> App deployment complete!"
elif [ "$_APP_DEPLOYED" = "true" ]; then
    echo "==> App was deployed during job run (endpoints were ready). All permissions now in place."
else
    echo "==> No app found in bundle, skipping app deployment."
fi
